{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d71bc261-6e78-4691-90ad-8e98eecc3221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import zipfile\n",
    "# from tqdm.auto import tqdm, trange # original\n",
    "from tqdm.notebook import tqdm, trange # fix for newline issue?\n",
    "from glob import glob\n",
    "from joblib import delayed, Parallel\n",
    "import matplotlib.pyplot as plt\n",
    "import torch_geometric as pyg\n",
    "import zipfile, os\n",
    "import torch\n",
    "import copy\n",
    "from torch import nn\n",
    "import networkx as nx\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm.auto import trange\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.neighbors import KNeighborsRegressor,RadiusNeighborsRegressor\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"Using device:\", device)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "import torch\n",
    "import torch_geometric as pyg\n",
    "from torch_geometric.loader import DataLoader # fix\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU, Sigmoid\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "import torch.optim as optim\n",
    "import joblib\n",
    "import gc\n",
    "from scipy.optimize import root_scalar\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "PATH_DATA0 = '../data/00.01'\n",
    "PATH_DATA = '../data/00.02'\n",
    "RANDOM_SEED =0\n",
    "np.random.seed(RANDOM_SEED)  \n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52d8520-ae75-4eb4-9861-d8180b66a491",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42fe8bb8-066b-4b6b-a8f6-54e8a90c992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CRITERION = nn.BCEWithLogitsLoss().to(device)\n",
    "LR = 0.001\n",
    "TOLERANCE = 20\n",
    "LR_TOLERANCE= 5\n",
    "MAX_EPOCHS = 1\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe91bb8-0c7c-48d2-a847-8d49edbb98fe",
   "metadata": {},
   "source": [
    "# Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c57214d9-6e6a-4ecf-ae09-238372e64216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader_train = pyg.loader.DataLoader(\n",
    "#     pd.read_pickle(os.path.join(PATH_DATA0, 'graphs','max_prob_10_subsample_0.1','graphs_train.pkl')).tolist(),\n",
    "#     batch_size = BATCH_SIZE,shuffle = True)\n",
    "# loader_val = pyg.loader.DataLoader(\n",
    "#     pd.read_pickle(os.path.join(PATH_DATA0, 'graphs','max_prob_10_subsample_0.1','graphs_val.pkl')).tolist(),batch_size = BATCH_SIZE\n",
    "#     ,shuffle = False)\n",
    "# loader_test = pyg.loader.DataLoader(\n",
    "#     pd.read_pickle(os.path.join(PATH_DATA0, 'graphs','max_prob_10_subsample_0.1','graphs_test.pkl')).tolist(),batch_size = BATCH_SIZE\n",
    "#     ,shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "149ef375",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = DataLoader(\n",
    "    pd.read_pickle(os.path.join(PATH_DATA0, 'graphs','max_prob_10_subsample_0.1','graphs_train.pkl')).tolist(),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "loader_val = DataLoader(\n",
    "    pd.read_pickle(os.path.join(PATH_DATA0, 'graphs','max_prob_10_subsample_0.1','graphs_val.pkl')).tolist(),\n",
    "    batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "loader_test = DataLoader(\n",
    "    pd.read_pickle(os.path.join(PATH_DATA0, 'graphs','max_prob_10_subsample_0.1','graphs_test.pkl')).tolist(),\n",
    "    batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2222d194-3ee1-42b0-998b-0c76b5ca8d0c",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc509bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x: 3 node features\n",
    "# edge_index: graph connectivity\n",
    "# edge_attr: unused here\n",
    "class DGCNN(nn.Module):\n",
    "    def __init__(self, num_node_features, num_edge_features, num_classes, num_layers=3, hidden_mult=1, device='cpu'):\n",
    "        super(DGCNN, self).__init__()\n",
    "        self.num_node_features = num_node_features\n",
    "        self.num_edge_features = num_edge_features\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.no_cuda = True if device == 'cpu' else True if self.device == 'mps' else False\n",
    "        self.hidden_mult = hidden_mult\n",
    "\n",
    "\n",
    "        # feature_dim_map = {\n",
    "        #     1: int(64 * hidden_mult),\n",
    "        #     2: int(128 * hidden_mult),\n",
    "        #     3: int(256 * hidden_mult),\n",
    "        #     4: int(512 * hidden_mult),\n",
    "        #     5: int(1024 * hidden_mult)\n",
    "        # }\n",
    "\n",
    "        # edgeconv_out_dim = 128\n",
    "        edgeconv_out_dim = self._ndim_map(hidden_mult if hidden_mult > 1 else 1)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        # self.layers.append(\n",
    "        #     pyg.nn.EdgeConv(nn.Sequential(\n",
    "        #         nn.Linear(num_node_features * 2, 64),\n",
    "        #         nn.ReLU(),\n",
    "        #         # nn.Linear(64, 128),\n",
    "        #         nn.Linear(64, edgeconv_out_dim),\n",
    "        #         nn.ReLU()\n",
    "        #     ))\n",
    "        # )\n",
    "        # subsequent EdgeConv layers\n",
    "        prev_out_dim = edgeconv_out_dim\n",
    "        for i in range(1, num_layers + 1):\n",
    "            print(f\"DEBUG: i = {i}\")\n",
    "            if i == 1 and num_layers > 1:\n",
    "                print(\"first layer, num_layers > 1\")\n",
    "                next_out_dim = self._ndim_map(i + 1)\n",
    "                self.layers.append(\n",
    "                    pyg.nn.EdgeConv(nn.Sequential(\n",
    "                        nn.Linear(num_node_features * 2, prev_out_dim),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(prev_out_dim, next_out_dim),\n",
    "                        nn.ReLU()\n",
    "                    ))\n",
    "                )\n",
    "                prev_out_dim = next_out_dim\n",
    "            elif i == 1 and num_layers == 1:\n",
    "                print(\"first layer, num_layers == 1\")\n",
    "                self.layers.append(\n",
    "                    pyg.nn.EdgeConv(nn.Sequential(\n",
    "                        nn.Linear(num_node_features * 2, prev_out_dim),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(prev_out_dim, 128),\n",
    "                        nn.ReLU()\n",
    "                    ))\n",
    "                )\n",
    "                # The output of this single layer is 128 dims\n",
    "                prev_out_dim = 128\n",
    "                edgeconv_out_dim = prev_out_dim  # keep edgeconv_out_dim for logging\n",
    "            elif i > 1 and i < num_layers:\n",
    "                print(\"middle layer\")\n",
    "                next_out_dim = self._ndim_map(i + 1)\n",
    "                self.layers.append(\n",
    "                    pyg.nn.EdgeConv(nn.Sequential(\n",
    "                        nn.Linear(prev_out_dim * 2, prev_out_dim),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(prev_out_dim, next_out_dim),\n",
    "                        nn.ReLU()\n",
    "                    ))\n",
    "                )\n",
    "                prev_out_dim = next_out_dim\n",
    "            elif i > 1 and  i == num_layers:\n",
    "                print(\"last layer\")\n",
    "                next_out_dim = self._ndim_map(i + 1)\n",
    "                self.layers.append(\n",
    "                    pyg.nn.EdgeConv(nn.Sequential(\n",
    "                        nn.Linear(prev_out_dim * 2, prev_out_dim),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(prev_out_dim, 128),\n",
    "                        nn.ReLU()\n",
    "                    ))\n",
    "                )\n",
    "                # update prev_out_dim to match this final layer's output\n",
    "                prev_out_dim = 128\n",
    "            else:\n",
    "                print(f\"DEBUG: i = {i}\")\n",
    "                print(\"Bad state in EdgeConv layer creation\")\n",
    "                # next_out_dim = self._ndim_map(i + 1)\n",
    "                # self.layers.append(\n",
    "                #     pyg.nn.EdgeConv(nn.Sequential(\n",
    "                #         nn.Linear(prev_out_dim, prev_out_dim),\n",
    "                #         nn.ReLU(),\n",
    "                #         nn.Linear(prev_out_dim, next_out_dim),\n",
    "                #         nn.ReLU()\n",
    "                #     ))\n",
    "                # )\n",
    "                # prev_out_dim = next_out_dim\n",
    "        print(f\"DEBUG: edgeconv_out_dim = {edgeconv_out_dim}\")\n",
    "        print(f\"DEBUG: prev_out_dim = {prev_out_dim}\")\n",
    "\n",
    "        # Edge-level fully connected layers\n",
    "        self.fc1 = nn.Linear(prev_out_dim * 2, 128, device=device)\n",
    "        self.fc2 = nn.Linear(128, num_classes, device=device)\n",
    "\n",
    "        # Move model to the designated device\n",
    "        self.to(device)\n",
    "\n",
    "    def _ndim_map(self, n):\n",
    "        # 1: 64\n",
    "        # 2: 128\n",
    "        # 3: 256\n",
    "        # 4: 512\n",
    "        # 5: 1024\n",
    "        # 6: 2048\n",
    "        # 7: 4096\n",
    "        if n < 1:\n",
    "            raise ValueError(\"n must be at least 1\")\n",
    "        if n % 1 != 0:\n",
    "            raise ValueError(\"n must be an integer\")\n",
    "        return 2**(5+n) * int(self.hidden_mult)\n",
    "\n",
    "    # def _init_weights(self, m):\n",
    "    #     if isinstance(m, nn.Linear):\n",
    "    #         nn.init.xavier_uniform_(m.weight)\n",
    "    #         if m.bias is not None:\n",
    "    #             nn.init.zeros_(m.bias)\n",
    "    #     elif isinstance(m, nn.Conv1d):\n",
    "    #         nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "    #         if m.bias is not None:\n",
    "    #             nn.init.zeros_(m.bias)\n",
    "    #     elif isinstance(m, nn.BatchNorm1d):\n",
    "    #         nn.init.ones_(m.weight)\n",
    "    #         nn.init.zeros_(m.bias)\n",
    "    #     elif isinstance(m, nn.LayerNorm):\n",
    "    #         nn.init.ones_(m.weight)\n",
    "    #         nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x.to(self.device), data.edge_index.to(self.device)\n",
    "\n",
    "        # Apply EdgeConv layers to get node embeddings\n",
    "        for conv in self.layers[:self.num_layers]:\n",
    "            x = conv(x, edge_index)\n",
    "\n",
    "        # Edge-level predictions: concatenate features of each edge's endpoints\n",
    "        row, col = edge_index\n",
    "        edge_emb = torch.cat([x[row], x[col]], dim=1)\n",
    "\n",
    "        # Pass through MLP\n",
    "        edge_h = F.relu(self.fc1(edge_emb))\n",
    "        edge_out = self.fc2(edge_h)\n",
    "\n",
    "        # Return one output per edge\n",
    "        return torch.sigmoid(edge_out).squeeze(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf055fc6-4688-4670-8bc5-907010be4eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RelationalModel(nn.Module):\n",
    "#     def __init__(self, input_size, output_size, hidden_size, n_layers):\n",
    "#         super(RelationalModel, self).__init__()\n",
    "\n",
    "#         layers = [nn.Linear(input_size, hidden_size), \n",
    "#                  nn.ReLU()]\n",
    "#         if n_layers>=2:\n",
    "#             for _ in range(n_layers - 1):\n",
    "#                 layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "#                 layers.append(nn.ReLU())\n",
    "\n",
    "#         layers.append(nn.Linear(hidden_size, output_size))\n",
    "\n",
    "#         self.layers = nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.layers(x)\n",
    "# class ObjectModel(nn.Module):\n",
    "#     def __init__(self, input_size, output_size, hidden_size, n_layers):\n",
    "#         super(ObjectModel, self).__init__()\n",
    "\n",
    "#         layers = [nn.Linear(input_size, hidden_size), \n",
    "#                  nn.ReLU()]\n",
    "#         if n_layers>=2:\n",
    "#             for _ in range(n_layers - 1):\n",
    "#                 layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "#                 layers.append(nn.ReLU())\n",
    "\n",
    "#         layers.append(nn.Linear(hidden_size, output_size))\n",
    "\n",
    "#         self.layers = nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, C):\n",
    "#         return self.layers(C)\n",
    "# class InteractionNetwork(MessagePassing):\n",
    "#     def __init__(self, hidden_size, n_layers):\n",
    "#         super(InteractionNetwork, self).__init__(aggr='add', \n",
    "#                                                  flow='source_to_target')\n",
    "#         self.R1 = RelationalModel(10, 4, hidden_size, n_layers)\n",
    "#         self.O = ObjectModel(7, 3, hidden_size, n_layers)\n",
    "#         self.R2 = RelationalModel(10, 1, hidden_size, n_layers)\n",
    "#         self.E: Tensor = Tensor()\n",
    "\n",
    "#     def forward(self, x: Tensor, edge_index: Tensor, edge_attr: Tensor) -> Tensor:\n",
    "\n",
    "#         # propagate_type: (x: Tensor, edge_attr: Tensor)\n",
    "#         x_tilde = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=None)\n",
    "\n",
    "#         m2 = torch.cat([x_tilde[edge_index[1]],\n",
    "#                         x_tilde[edge_index[0]],\n",
    "#                         self.E], dim=1)\n",
    "#         return self.R2(m2)\n",
    "\n",
    "#     def message(self, x_i, x_j, edge_attr):\n",
    "#         # x_i --> incoming\n",
    "#         # x_j --> outgoing        \n",
    "#         m1 = torch.cat([x_i, x_j, edge_attr], dim=1)\n",
    "#         self.E = self.R1(m1)\n",
    "#         return self.E\n",
    "\n",
    "#     def update(self, aggr_out, x):\n",
    "#         c = torch.cat([x, aggr_out], dim=1)\n",
    "#         return self.O(c) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e474f636-44c5-4064-b74d-2715dfe92888",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf42440-53fd-4ca4-84de-2752fcacb1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return trainable_params\n",
    "\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {path}\")\n",
    "    return model\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, actuals = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch)\n",
    "            # turn node-y into graph-y\n",
    "            y_nodes = batch.y.float().unsqueeze(-1)  # [num_nodes,1]\n",
    "            # manual pooling using batch.ptr\n",
    "            ptr = batch.ptr  # shape [num_graphs+1]\n",
    "            # compute mean of y_nodes for each graph\n",
    "            y_graph = torch.cat([\n",
    "                y_nodes[ptr[i]:ptr[i+1]].mean(dim=0, keepdim=True)\n",
    "                for i in range(ptr.size(0) - 1)\n",
    "            ], dim=0).squeeze(-1)\n",
    "            preds.append(out)\n",
    "            actuals.append(y_graph)\n",
    "        preds = torch.cat(preds, dim=0)\n",
    "        actuals = torch.cat(actuals, dim=0)\n",
    "        acc = ((torch.sigmoid(preds)>0.5)==(actuals>0.5)).float().mean().item()\n",
    "        print([(p.item(), a.item()) for p, a in zip(preds, actuals)])\n",
    "        print(f\"preds: {preds}, actuals: {actuals}, acc: {acc}\")\n",
    "        entropy = CRITERION(preds, actuals).item()\n",
    "    model.train()\n",
    "    return preds.cpu().numpy(), actuals.cpu().numpy(), acc, entropy\n",
    "\n",
    "def train_epoch(model, loader_train, optimizer):\n",
    "    model.train()\n",
    "    print(\"Running on \", next(model.parameters()).device)\n",
    "    train_loss = 0.0\n",
    "    for batch in tqdm(loader_train, leave=False):\n",
    "        batch = batch.to(device)\n",
    "        # Ensure key graph tensors are on the target device\n",
    "        if hasattr(batch, 'x') and isinstance(batch.x, torch.Tensor):\n",
    "            assert str(batch.x.device).startswith(str(device)), f\"batch.x on {batch.x.device}, expected {device}\"\n",
    "        if hasattr(batch, 'edge_index') and isinstance(batch.edge_index, torch.Tensor):\n",
    "            assert str(batch.edge_index.device).startswith(str(device)), f\"batch.edge_index on {batch.edge_index.device}, expected {device}\"\n",
    "        if hasattr(batch, 'edge_attr') and isinstance(batch.edge_attr, torch.Tensor):\n",
    "            assert str(batch.edge_attr.device).startswith(str(device)), f\"batch.edge_attr on {batch.edge_attr.device}, expected {device}\"\n",
    "        if hasattr(batch, 'batch') and isinstance(batch.batch, torch.Tensor):\n",
    "            assert str(batch.batch.device).startswith(str(device)), f\"batch.batch on {batch.batch.device}, expected {device}\"\n",
    "        if hasattr(batch, 'y') and isinstance(batch.y, torch.Tensor):\n",
    "            assert str(batch.y.device).startswith(str(device)), f\"batch.y on {batch.y.device}, expected {device}\"\n",
    "        optimizer.zero_grad()\n",
    "        print(f\"batch: {batch}\")\n",
    "        out = model(batch)                # [batch_size,1]\n",
    "        # graph-level target\n",
    "        y_nodes = batch.y.float().unsqueeze(-1)     # [num_nodes,1]\n",
    "        y_edges = batch.y.float()     # [num_edges,1]\n",
    "        # manual pooling using batch.ptr\n",
    "        ptr = batch.ptr\n",
    "        y_graph = torch.cat([\n",
    "            y_nodes[ptr[i]:ptr[i+1]].mean(dim=0, keepdim=True)\n",
    "            for i in range(ptr.size(0) - 1)\n",
    "        ], dim=0).squeeze(-1)\n",
    "        print(f\"out: {out}, y_graph: {y_graph}\")\n",
    "        print(f\"out.shape: {out.shape}, y_graph.shape: {y_graph.shape}\")\n",
    "        loss = CRITERION(out, y_graph)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * batch.num_graphs\n",
    "    return train_loss / len(loader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ab3ce22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17826812\n"
     ]
    }
   ],
   "source": [
    "data = loader_train.dataset\n",
    "n = 0\n",
    "for g in data:\n",
    "    n += len(g.y)\n",
    "\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3430db76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Number of parameters in model:  91201\n"
     ]
    }
   ],
   "source": [
    "N_LAYERS = 2\n",
    "HIDDEN_MULT = 1\n",
    "\n",
    "# Numper of parameters\n",
    "print(\"Number of parameters in model: \", count_parameters(DGCNN(3, 4, 1, N_LAYERS, HIDDEN_MULT).to(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff5c058-e13a-4203-9aff-6d8706678e06",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70d49b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "ModuleList(\n",
      "  (0): EdgeConv(nn=Sequential(\n",
      "    (0): Linear(in_features=6, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "  ))\n",
      "  (1): EdgeConv(nn=Sequential(\n",
      "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "  ))\n",
      ")\n",
      "Running on  mps:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52efb5bd0d064a4b94cecbad203e6c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: DataBatch(x=[88156, 3], edge_index=[2, 1014522], edge_attr=[1014522, 4], y=[1014522, 1], hit_ids=[88156], particle_ids=[88156], batch=[88156], ptr=[33])\n",
      "out: tensor([0.4910, 0.4914, 0.4915,  ..., 0.4907, 0.4902, 0.4896], device='mps:0',\n",
      "       grad_fn=<SqueezeBackward1>), y_graph: tensor([[0.9450],\n",
      "        [0.8643],\n",
      "        [0.7682],\n",
      "        [0.8444],\n",
      "        [0.9125],\n",
      "        [0.9763],\n",
      "        [0.9294],\n",
      "        [0.8789],\n",
      "        [0.8783],\n",
      "        [0.8528],\n",
      "        [0.9524],\n",
      "        [0.8957],\n",
      "        [0.6589],\n",
      "        [0.7381],\n",
      "        [0.7786],\n",
      "        [0.8345],\n",
      "        [0.9217],\n",
      "        [0.9311],\n",
      "        [0.8427],\n",
      "        [0.8317],\n",
      "        [0.8415],\n",
      "        [0.8161],\n",
      "        [0.8915],\n",
      "        [0.7960],\n",
      "        [0.8633],\n",
      "        [0.9693],\n",
      "        [0.9366],\n",
      "        [0.6261],\n",
      "        [0.7637],\n",
      "        [0.7911],\n",
      "        [0.8473],\n",
      "        [0.9631]], device='mps:0')\n",
      "out.shape: torch.Size([1014522]), y_graph.shape: torch.Size([32, 1])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([32, 1])) must be the same as input size (torch.Size([1014522]))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(all_preds, dim=\u001b[32m0\u001b[39m)\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m model, train_losses, val_accs, val_entropies = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDGCNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_node_features\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_edge_features\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_LAYERS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_mult\u001b[49m\u001b[43m=\u001b[49m\u001b[43mHIDDEN_MULT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloader_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloader_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloader_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloader_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLR\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, loader_train, loader_val, num_epochs, lr)\u001b[39m\n\u001b[32m     11\u001b[39m val_entropies = []\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     val_preds, val_actuals, val_acc, val_entropy = evaluate(model, loader_val)\n\u001b[32m     15\u001b[39m     train_losses.append(train_loss)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, loader_train, optimizer)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mout: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, y_graph: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_graph\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     71\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mout.shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, y_graph.shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_graph.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m loss = \u001b[43mCRITERION\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m loss.backward()\n\u001b[32m     74\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/torch/nn/modules/loss.py:821\u001b[39m, in \u001b[36mBCEWithLogitsLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m    820\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/torch/nn/functional.py:3639\u001b[39m, in \u001b[36mbinary_cross_entropy_with_logits\u001b[39m\u001b[34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[39m\n\u001b[32m   3636\u001b[39m     reduction_enum = _Reduction.get_enum(reduction)\n\u001b[32m   3638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target.size() == \u001b[38;5;28minput\u001b[39m.size()):\n\u001b[32m-> \u001b[39m\u001b[32m3639\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3640\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3641\u001b[39m     )\n\u001b[32m   3643\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.binary_cross_entropy_with_logits(\n\u001b[32m   3644\u001b[39m     \u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum\n\u001b[32m   3645\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Target size (torch.Size([32, 1])) must be the same as input size (torch.Size([1014522]))"
     ]
    }
   ],
   "source": [
    "# Simplified training loop\n",
    "def train_model(model, loader_train, loader_val, num_epochs=MAX_EPOCHS, lr=LR):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    # for name, p in model.named_parameters():\n",
    "    #     print(f\"{name}: {p.device}\")\n",
    "    print(model.layers)\n",
    "    best_val_acc = 0.0\n",
    "    best_model = None\n",
    "    train_losses = []\n",
    "    val_accs = []\n",
    "    val_entropies = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_epoch(model, loader_train, optimizer)\n",
    "        val_preds, val_actuals, val_acc, val_entropy = evaluate(model, loader_val)\n",
    "        train_losses.append(train_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        val_entropies.append(val_entropy)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "            save_model(best_model, os.path.join(PATH_DATA, f\"best_model_epoch_{epoch+1}.pth\"))\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Acc: {val_acc:.4f}, Val Entropy: {val_entropy:.4f}\")\n",
    "\n",
    "        # for name, p in model.named_parameters():\n",
    "        #     print(f\"{name}: {p.device}\")\n",
    "    return best_model, train_losses, val_accs, val_entropies\n",
    "\n",
    "def plot_training(train_losses, val_accs, val_entropies):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(val_accs, label='Val Acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(val_entropies, label='Val Entropy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Entropy')\n",
    "    plt.title('Validation Entropy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {path}\")\n",
    "\n",
    "def predict(model, loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            preds = model(data)\n",
    "            all_preds.append(preds)\n",
    "    return torch.cat(all_preds, dim=0)\n",
    "\n",
    "# Run training\n",
    "model, train_losses, val_accs, val_entropies = train_model(\n",
    "    model=DGCNN(num_node_features=3, num_edge_features=4, num_classes=1, num_layers=N_LAYERS, hidden_mult=HIDDEN_MULT, device=device),\n",
    "    loader_train=loader_train,\n",
    "    loader_val=loader_val,\n",
    "    num_epochs=MAX_EPOCHS,\n",
    "    lr=LR\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5aabb44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Number of parameters in the model: 74817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): EdgeConv(nn=Sequential(\n",
       "    (0): Linear(in_features=6, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "  ))\n",
       "  (1): EdgeConv(nn=Sequential(\n",
       "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "  ))\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DGCNN(num_node_features=3, num_edge_features=4, num_classes=1, device=device, num_layers=3, hidden_mult=1)\n",
    "print(f\"Number of parameters in the model: {count_parameters(model)}\")\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2579cb8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c759e11c53bb466f90470188be26c966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b69e986c0c48daaf45a5629d21b9a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: 2\n",
      "Target Params: 100000\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 1.0, Params: -25183\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 2048\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 4.0, Params: 1570529\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.5, Params: 148449\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 1.75, Params: -25183\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.125, Params: 148449\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 1.9375, Params: -25183\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.03125, Params: 148449\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 1.984375, Params: -25183\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.0078125, Params: 148449\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 1.99609375, Params: -25183\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.001953125, Params: 148449\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 1.9990234375, Params: -25183\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.00048828125, Params: 148449\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 1.999755859375, Params: -25183\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.0001220703125, Params: 148449\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 1.99993896484375, Params: -25183\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.000030517578125, Params: 148449\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 1.9999847412109375, Params: -25183\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.0000076293945312, Params: 148449\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 1.9999961853027344, Params: -25183\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.000001907348633, Params: 148449\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 1.9999990463256836, Params: -25183\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.000000476837158, Params: 148449\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 1.999999761581421, Params: -25183\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.0000001192092896, Params: 148449\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 1.9999999403953552, Params: -25183\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.0000000298023224, Params: 148449\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 1.9999999850988388, Params: -25183\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.0000000074505806, Params: 148449\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 1.9999999962747097, Params: -25183\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.000000001862645, Params: 148449\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 1.9999999990686774, Params: -25183\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.0000000004656613, Params: 148449\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 1.9999999997671694, Params: -25183\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.0000000001164153, Params: 148449\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 1.9999999999417923, Params: -25183\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.000000000029104, Params: 148449\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 1.999999999985448, Params: -25183\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.000000000007276, Params: 148449\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 1.999999999996362, Params: -25183\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.000000000001819, Params: 148449\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 1.9999999999990905, Params: -25183\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.0000000000004547, Params: 148449\n",
      "(1) Optimal h: 2\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Model Summary:\n",
      "--------------------------------------------------\n",
      "Model Type: DGCNN\n",
      "Number of Layers: 2\n",
      "Number of Parameters: 248449\n",
      "Device: mps\n",
      "--------------------------------------------------\n",
      "Layer Details:\n",
      "ModuleList(\n",
      "  (0): EdgeConv(nn=Sequential(\n",
      "    (0): Linear(in_features=6, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "  ))\n",
      "  (1): EdgeConv(nn=Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "  ))\n",
      ")\n",
      "Epoch | Train Loss |  Val Loss |  Val Acc |  Test Acc\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d1e07d589340ceaf557c6b6f88f0ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on  mps:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d65fe7004734f8885842f384232054a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.0629],\n",
      "        [0.0625],\n",
      "        [0.0626],\n",
      "        [0.0628],\n",
      "        [0.0629],\n",
      "        [0.0629],\n",
      "        [0.0626],\n",
      "        [0.0628],\n",
      "        [0.0630],\n",
      "        [0.0632],\n",
      "        [0.0628],\n",
      "        [0.0628],\n",
      "        [0.0631],\n",
      "        [0.0631],\n",
      "        [0.0627],\n",
      "        [0.0627],\n",
      "        [0.0627],\n",
      "        [0.0626],\n",
      "        [0.0627],\n",
      "        [0.0630],\n",
      "        [0.0629],\n",
      "        [0.0626],\n",
      "        [0.0630],\n",
      "        [0.0630],\n",
      "        [0.0630],\n",
      "        [0.0625],\n",
      "        [0.0631],\n",
      "        [0.0628],\n",
      "        [0.0625],\n",
      "        [0.0630],\n",
      "        [0.0628],\n",
      "        [0.0627]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.1404],\n",
      "        [0.1436],\n",
      "        [0.1395],\n",
      "        [0.1418],\n",
      "        [0.1390],\n",
      "        [0.1399],\n",
      "        [0.1392],\n",
      "        [0.1403],\n",
      "        [0.1402],\n",
      "        [0.1408],\n",
      "        [0.1382],\n",
      "        [0.1399],\n",
      "        [0.1394],\n",
      "        [0.1408],\n",
      "        [0.1405],\n",
      "        [0.1388],\n",
      "        [0.1401],\n",
      "        [0.1406],\n",
      "        [0.1410],\n",
      "        [0.1415],\n",
      "        [0.1409],\n",
      "        [0.1401],\n",
      "        [0.1392],\n",
      "        [0.1399],\n",
      "        [0.1416],\n",
      "        [0.1399],\n",
      "        [0.1410],\n",
      "        [0.1399],\n",
      "        [0.1410],\n",
      "        [0.1411],\n",
      "        [0.1398],\n",
      "        [0.1404]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.2241],\n",
      "        [0.2254],\n",
      "        [0.2211],\n",
      "        [0.2217],\n",
      "        [0.2251],\n",
      "        [0.2215],\n",
      "        [0.2235],\n",
      "        [0.2202],\n",
      "        [0.2223],\n",
      "        [0.2217],\n",
      "        [0.2224],\n",
      "        [0.2231],\n",
      "        [0.2240],\n",
      "        [0.2251],\n",
      "        [0.2208],\n",
      "        [0.2203],\n",
      "        [0.2228],\n",
      "        [0.2251],\n",
      "        [0.2246],\n",
      "        [0.2232],\n",
      "        [0.2229],\n",
      "        [0.2228],\n",
      "        [0.2266],\n",
      "        [0.2232],\n",
      "        [0.2240],\n",
      "        [0.2235],\n",
      "        [0.2271],\n",
      "        [0.2183],\n",
      "        [0.2237],\n",
      "        [0.2271],\n",
      "        [0.2244],\n",
      "        [0.2228]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.3429],\n",
      "        [0.3413],\n",
      "        [0.3448],\n",
      "        [0.3404],\n",
      "        [0.3410],\n",
      "        [0.3421],\n",
      "        [0.3383],\n",
      "        [0.3412],\n",
      "        [0.3379],\n",
      "        [0.3409],\n",
      "        [0.3410],\n",
      "        [0.3473],\n",
      "        [0.3510],\n",
      "        [0.3489],\n",
      "        [0.3429],\n",
      "        [0.3406],\n",
      "        [0.3403],\n",
      "        [0.3472],\n",
      "        [0.3398],\n",
      "        [0.3463],\n",
      "        [0.3420],\n",
      "        [0.3386],\n",
      "        [0.3398],\n",
      "        [0.3399],\n",
      "        [0.3424],\n",
      "        [0.3418],\n",
      "        [0.3435],\n",
      "        [0.3445],\n",
      "        [0.3399],\n",
      "        [0.3393],\n",
      "        [0.3383],\n",
      "        [0.3386]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.5347],\n",
      "        [0.5340],\n",
      "        [0.5250],\n",
      "        [0.5318],\n",
      "        [0.5220],\n",
      "        [0.5310],\n",
      "        [0.5305],\n",
      "        [0.5347],\n",
      "        [0.5204],\n",
      "        [0.5340],\n",
      "        [0.5240],\n",
      "        [0.5322],\n",
      "        [0.5309],\n",
      "        [0.5357],\n",
      "        [0.5262],\n",
      "        [0.5377],\n",
      "        [0.5319],\n",
      "        [0.5329],\n",
      "        [0.5312],\n",
      "        [0.5321],\n",
      "        [0.5266],\n",
      "        [0.5396],\n",
      "        [0.5375],\n",
      "        [0.5353],\n",
      "        [0.5317],\n",
      "        [0.5197],\n",
      "        [0.5411],\n",
      "        [0.5344],\n",
      "        [0.5387],\n",
      "        [0.5276],\n",
      "        [0.5354],\n",
      "        [0.5353]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.8513],\n",
      "        [0.8330],\n",
      "        [0.8355],\n",
      "        [0.8568],\n",
      "        [0.8268],\n",
      "        [0.8197],\n",
      "        [0.8371],\n",
      "        [0.8460],\n",
      "        [0.8241],\n",
      "        [0.8566],\n",
      "        [0.8204],\n",
      "        [0.8499],\n",
      "        [0.8316],\n",
      "        [0.8065],\n",
      "        [0.8362],\n",
      "        [0.8192],\n",
      "        [0.8373],\n",
      "        [0.8462],\n",
      "        [0.8428],\n",
      "        [0.8514],\n",
      "        [0.8365],\n",
      "        [0.8298],\n",
      "        [0.8385],\n",
      "        [0.8251],\n",
      "        [0.8419],\n",
      "        [0.8578],\n",
      "        [0.8330],\n",
      "        [0.8308],\n",
      "        [0.8316],\n",
      "        [0.8440],\n",
      "        [0.8512],\n",
      "        [0.8362]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[1.2429],\n",
      "        [1.2488],\n",
      "        [1.2343],\n",
      "        [1.2260],\n",
      "        [1.2193],\n",
      "        [1.2197],\n",
      "        [1.2396],\n",
      "        [1.2608],\n",
      "        [1.2343],\n",
      "        [1.2088],\n",
      "        [1.2170],\n",
      "        [1.2117],\n",
      "        [1.2129],\n",
      "        [1.2210],\n",
      "        [1.2508],\n",
      "        [1.2287],\n",
      "        [1.2119],\n",
      "        [1.2331],\n",
      "        [1.2188],\n",
      "        [1.2208],\n",
      "        [1.2624],\n",
      "        [1.2434],\n",
      "        [1.2626],\n",
      "        [1.2232],\n",
      "        [1.2362],\n",
      "        [1.2671],\n",
      "        [1.2259],\n",
      "        [1.2411],\n",
      "        [1.2072],\n",
      "        [1.2226],\n",
      "        [1.2135],\n",
      "        [1.2503]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[1.8395],\n",
      "        [1.8279],\n",
      "        [1.8359],\n",
      "        [1.8258],\n",
      "        [1.7911],\n",
      "        [1.8367],\n",
      "        [1.8407],\n",
      "        [1.8196],\n",
      "        [1.8251],\n",
      "        [1.8015],\n",
      "        [1.8378],\n",
      "        [1.8230],\n",
      "        [1.8504],\n",
      "        [1.8373],\n",
      "        [1.8657],\n",
      "        [1.8620],\n",
      "        [1.8462],\n",
      "        [1.7875],\n",
      "        [1.8030],\n",
      "        [1.8522],\n",
      "        [1.8180],\n",
      "        [1.8341],\n",
      "        [1.8232],\n",
      "        [1.8560],\n",
      "        [1.8203],\n",
      "        [1.8319],\n",
      "        [1.8199],\n",
      "        [1.8395],\n",
      "        [1.8589],\n",
      "        [1.8442],\n",
      "        [1.8062],\n",
      "        [1.8221]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[2.2939],\n",
      "        [2.2678],\n",
      "        [2.2848],\n",
      "        [2.2493],\n",
      "        [2.2900],\n",
      "        [2.3105],\n",
      "        [2.2966],\n",
      "        [2.2889],\n",
      "        [2.2465],\n",
      "        [2.2134],\n",
      "        [2.2855],\n",
      "        [2.2582],\n",
      "        [2.1823],\n",
      "        [2.3095],\n",
      "        [2.3257],\n",
      "        [2.2994],\n",
      "        [2.2703],\n",
      "        [2.2674],\n",
      "        [2.3226],\n",
      "        [2.2477],\n",
      "        [2.2897],\n",
      "        [2.2518],\n",
      "        [2.2767],\n",
      "        [2.2984],\n",
      "        [2.3047],\n",
      "        [2.3087],\n",
      "        [2.3088],\n",
      "        [2.2840],\n",
      "        [2.3179],\n",
      "        [2.2721],\n",
      "        [2.2662],\n",
      "        [2.2673]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135],\n",
      "        [0.1135]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191],\n",
      "        [0.1191]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249],\n",
      "        [0.1249]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371],\n",
      "        [0.1371]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434],\n",
      "        [0.1434]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498],\n",
      "        [0.1498]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([22, 128])\n",
      "DEBUG: x_out shape = torch.Size([22, 1])\n",
      "tensor([[0.1562],\n",
      "        [0.1562],\n",
      "        [0.1562],\n",
      "        [0.1562],\n",
      "        [0.1562],\n",
      "        [0.1562],\n",
      "        [0.1562],\n",
      "        [0.1562],\n",
      "        [0.1562],\n",
      "        [0.1562],\n",
      "        [0.1562],\n",
      "        [0.1562],\n",
      "        [0.1562],\n",
      "        [0.1562],\n",
      "        [0.1562],\n",
      "        [0.1562],\n",
      "        [0.1562],\n",
      "        [0.1562],\n",
      "        [0.1562],\n",
      "        [0.1562],\n",
      "        [0.1562],\n",
      "        [0.1562]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627]], device='mps:0')\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627]], device='mps:0')\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627]], device='mps:0')\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627]], device='mps:0')\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([13, 128])\n",
      "DEBUG: x_out shape = torch.Size([13, 1])\n",
      "tensor([[0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627]], device='mps:0')\n",
      "[(0.5405843257904053, 0.9693124890327454), (0.5405843257904053, 0.8620213866233826), (0.5405843257904053, 0.6552368998527527), (0.5405843257904053, 0.731358528137207), (0.5405843257904053, 0.8018386363983154), (0.5405843257904053, 0.8518771529197693), (0.5405843257904053, 0.9593373537063599), (0.5405843257904053, 0.9171882271766663), (0.5405843257904053, 0.8471965789794922), (0.5405843257904053, 0.8326959609985352), (0.5405843257904053, 0.8448637127876282), (0.5405843257904053, 0.8169826865196228), (0.5405843257904053, 0.7986486554145813), (0.5405843257904053, 0.7748074531555176), (0.5405843257904053, 0.9732722640037537), (0.5405843257904053, 0.863972008228302), (0.5405843257904053, 0.6556621789932251), (0.5405843257904053, 0.7748690843582153), (0.5405843257904053, 0.7977927923202515), (0.5405843257904053, 0.8850793838500977), (0.5405843257904053, 0.9498903751373291), (0.5405843257904053, 0.856199324131012), (0.5405843257904053, 0.8407439589500427), (0.5405843257904053, 0.8880537748336792), (0.5405843257904053, 0.8424956202507019), (0.5405843257904053, 0.8765586018562317), (0.5405843257904053, 0.9122468829154968), (0.5405843257904053, 0.7174603343009949), (0.5405843257904053, 0.8038022518157959), (0.5405843257904053, 0.837803304195404), (0.5405843257904053, 0.8788526058197021), (0.5405843257904053, 0.9711704254150391), (0.5405843257904053, 0.9682285189628601), (0.5405843257904053, 0.6585366129875183), (0.5405843257904053, 0.7271094918251038), (0.5405843257904053, 0.7597004771232605), (0.5405843257904053, 0.817583441734314), (0.5405843257904053, 0.8725566864013672), (0.5405843257904053, 0.9391411542892456), (0.5405843257904053, 0.8788377046585083), (0.5405843257904053, 0.8283891677856445), (0.5405843257904053, 0.8036096692085266), (0.5405843257904053, 0.7964332103729248), (0.5405843257904053, 0.859919548034668), (0.5405843257904053, 0.6555455327033997), (0.5405843257904053, 0.9453434944152832), (0.5405843257904053, 0.7439261674880981), (0.5405843257904053, 0.6901408433914185), (0.5405843257904053, 0.7337499856948853), (0.5405843257904053, 0.7436479330062866), (0.5405843257904053, 0.7791483998298645), (0.5405843257904053, 0.9252336621284485), (0.5405843257904053, 0.9445716738700867), (0.5405843257904053, 0.8969812989234924), (0.5405843257904053, 0.8264392614364624), (0.5405843257904053, 0.8039056062698364), (0.5405843257904053, 0.7962427735328674), (0.5405843257904053, 0.9156963229179382), (0.5405843257904053, 0.698814868927002), (0.5405843257904053, 0.8855873346328735), (0.5405843257904053, 0.9652000069618225), (0.5405843257904053, 0.783442497253418), (0.5405843257904053, 0.7892302870750427), (0.5405843257904053, 0.8746948838233948), (0.5405843257904053, 0.9515347480773926), (0.5405843257904053, 0.7957894802093506), (0.5405843257904053, 0.6033783555030823), (0.5405843257904053, 0.7250791192054749), (0.5405843257904053, 0.8125210404396057), (0.5405843257904053, 0.9465240836143494), (0.5405843257904053, 0.8380345702171326), (0.5405843257904053, 0.8102991580963135), (0.5405843257904053, 0.7858898043632507), (0.5405843257904053, 0.847517728805542), (0.5405843257904053, 0.7637944221496582), (0.5405843257904053, 0.9547445178031921), (0.5405843257904053, 0.7909984588623047), (0.5405843257904053, 0.6597691774368286), (0.5405843257904053, 0.826769232749939), (0.5405843257904053, 0.8571428656578064), (0.5405843257904053, 0.9403994679450989), (0.5405843257904053, 0.8992273211479187), (0.5405843257904053, 0.8707733750343323), (0.5405843257904053, 0.8826386332511902), (0.5405843257904053, 0.8948948979377747), (0.5405843257904053, 0.7912303805351257), (0.5405843257904053, 0.8672150373458862), (0.5405843257904053, 0.9360671639442444), (0.5405843257904053, 0.640816330909729), (0.5405843257904053, 0.7863070368766785), (0.5405843257904053, 0.8247955441474915), (0.5405843257904053, 0.830997884273529), (0.5405843257904053, 0.9440909028053284), (0.5405843257904053, 0.9701545238494873), (0.5405843257904053, 0.9424275755882263), (0.5405843257904053, 0.8683087229728699), (0.5405843257904053, 0.9862015247344971), (0.5405843257904053, 0.9555920958518982), (0.5405843257904053, 0.6087948083877563), (0.5405843257904053, 0.7294608354568481), (0.5405843257904053, 0.7664744853973389), (0.5405843257904053, 0.7813860177993774), (0.5405843257904053, 0.8207758665084839), (0.5405843257904053, 0.9776926040649414), (0.5405843257904053, 0.8775661587715149), (0.5405843257904053, 0.8455030918121338), (0.5405843257904053, 0.8279106020927429), (0.5405843257904053, 0.830202579498291), (0.5405843257904053, 0.8081476092338562), (0.5405843257904053, 0.8753846287727356), (0.5405843257904053, 0.7257839441299438), (0.5405843257904053, 0.8398544192314148), (0.5405843257904053, 0.828998327255249), (0.5405843257904053, 0.8525398969650269), (0.5405843257904053, 0.8915207982063293), (0.5405843257904053, 0.9050298929214478), (0.5405843257904053, 0.9175897240638733), (0.5405843257904053, 0.9156056642532349), (0.5405843257904053, 0.9323220252990723), (0.5405843257904053, 0.9560975432395935), (0.5405843257904053, 0.7887994050979614), (0.5405843257904053, 0.7424344420433044), (0.5405843257904053, 0.8212013840675354), (0.5405843257904053, 0.866995096206665), (0.5405843257904053, 0.9692716002464294), (0.5405843257904053, 0.9469627141952515), (0.5405843257904053, 0.894755482673645), (0.5405843257904053, 0.8676123023033142), (0.5405843257904053, 0.9555655121803284), (0.5405843257904053, 0.9140759706497192), (0.5405843257904053, 0.7625056505203247), (0.5405843257904053, 0.8154761791229248), (0.5405843257904053, 0.8497784733772278), (0.5405843257904053, 0.9638336300849915), (0.5405843257904053, 0.9130056500434875), (0.5405843257904053, 0.8544891476631165), (0.5405843257904053, 0.8389671444892883), (0.5405843257904053, 0.8575875759124756), (0.5405843257904053, 0.8661844730377197), (0.5405843257904053, 0.7827287316322327), (0.5405843257904053, 0.6995591521263123)]\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627]], device='mps:0')\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627]], device='mps:0')\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627]], device='mps:0')\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627]], device='mps:0')\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627]], device='mps:0')\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([17, 128])\n",
      "DEBUG: x_out shape = torch.Size([17, 1])\n",
      "tensor([[0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627],\n",
      "        [0.1627]], device='mps:0')\n",
      "[(0.5405843257904053, 0.9655052423477173), (0.5405843257904053, 0.8504638075828552), (0.5405843257904053, 0.7599217295646667), (0.5405843257904053, 0.8221729397773743), (0.5405843257904053, 0.8905238509178162), (0.5405843257904053, 0.9811320900917053), (0.5405843257904053, 0.9476417303085327), (0.5405843257904053, 0.9091302752494812), (0.5405843257904053, 0.8846827745437622), (0.5405843257904053, 0.8751519918441772), (0.5405843257904053, 0.9135047197341919), (0.5405843257904053, 0.8045705556869507), (0.5405843257904053, 0.9394440054893494), (0.5405843257904053, 0.709606409072876), (0.5405843257904053, 0.7598899006843567), (0.5405843257904053, 0.7772151827812195), (0.5405843257904053, 0.9030411243438721), (0.5405843257904053, 0.9588112831115723), (0.5405843257904053, 0.863008975982666), (0.5405843257904053, 0.8421828746795654), (0.5405843257904053, 0.8144736886024475), (0.5405843257904053, 0.8170731663703918), (0.5405843257904053, 0.8231569528579712), (0.5405843257904053, 0.8529317378997803), (0.5405843257904053, 0.9674796462059021), (0.5405843257904053, 0.9296508431434631), (0.5405843257904053, 0.6805661916732788), (0.5405843257904053, 0.7108249068260193), (0.5405843257904053, 0.7444615960121155), (0.5405843257904053, 0.7717621922492981), (0.5405843257904053, 0.8266100287437439), (0.5405843257904053, 0.9611037373542786), (0.5405843257904053, 0.9618805646896362), (0.5405843257904053, 0.9341359734535217), (0.5405843257904053, 0.654036819934845), (0.5405843257904053, 0.7629629373550415), (0.5405843257904053, 0.7860445380210876), (0.5405843257904053, 0.8350680470466614), (0.5405843257904053, 0.9259384274482727), (0.5405843257904053, 0.9171465039253235), (0.5405843257904053, 0.8592531681060791), (0.5405843257904053, 0.8534271717071533), (0.5405843257904053, 0.8501827120780945), (0.5405843257904053, 0.8529411554336548), (0.5405843257904053, 0.751278281211853), (0.5405843257904053, 0.9731951355934143), (0.5405843257904053, 0.9569798111915588), (0.5405843257904053, 0.6710816621780396), (0.5405843257904053, 0.8302675485610962), (0.5405843257904053, 0.8435965180397034), (0.5405843257904053, 0.8735101819038391), (0.5405843257904053, 0.9378882050514221), (0.5405843257904053, 0.9367133975028992), (0.5405843257904053, 0.8918673396110535), (0.5405843257904053, 0.879772961139679), (0.5405843257904053, 0.8565649390220642), (0.5405843257904053, 0.8369829654693604), (0.5405843257904053, 0.8144654035568237), (0.5405843257904053, 0.855760395526886), (0.5405843257904053, 0.9720301032066345), (0.5405843257904053, 0.7513445019721985), (0.5405843257904053, 0.8287740349769592), (0.5405843257904053, 0.850024938583374), (0.5405843257904053, 0.938327431678772), (0.5405843257904053, 0.9709104895591736), (0.5405843257904053, 0.7878103852272034), (0.5405843257904053, 0.6736976504325867), (0.5405843257904053, 0.8126245737075806), (0.5405843257904053, 0.8235741257667542), (0.5405843257904053, 0.8951426148414612), (0.5405843257904053, 0.9753351211547852), (0.5405843257904053, 0.8842900395393372), (0.5405843257904053, 0.8652786016464233), (0.5405843257904053, 0.8318256735801697), (0.5405843257904053, 0.8387959599494934), (0.5405843257904053, 0.8739684820175171), (0.5405843257904053, 0.8671109080314636), (0.5405843257904053, 0.7538025975227356), (0.5405843257904053, 0.8261919021606445), (0.5405843257904053, 0.9088714718818665), (0.5405843257904053, 0.9616315364837646), (0.5405843257904053, 0.875), (0.5405843257904053, 0.8871315717697144), (0.5405843257904053, 0.8750445246696472), (0.5405843257904053, 0.8928571343421936), (0.5405843257904053, 0.9766656160354614), (0.5405843257904053, 0.9164619445800781), (0.5405843257904053, 0.7142336964607239), (0.5405843257904053, 0.8130204677581787), (0.5405843257904053, 0.8311800360679626), (0.5405843257904053, 0.8750593662261963), (0.5405843257904053, 0.9308005571365356), (0.5405843257904053, 0.9033641219139099), (0.5405843257904053, 0.8836386799812317), (0.5405843257904053, 0.8903133869171143), (0.5405843257904053, 0.8824875950813293), (0.5405843257904053, 0.9334677457809448), (0.5405843257904053, 0.8644501566886902), (0.5405843257904053, 0.6918863654136658), (0.5405843257904053, 0.7622100114822388), (0.5405843257904053, 0.806530237197876), (0.5405843257904053, 0.9087315797805786), (0.5405843257904053, 0.8956719636917114), (0.5405843257904053, 0.8333333134651184), (0.5405843257904053, 0.8378645777702332), (0.5405843257904053, 0.8525896668434143), (0.5405843257904053, 0.8154574036598206), (0.5405843257904053, 0.8736681938171387), (0.5405843257904053, 0.8048909902572632), (0.5405843257904053, 0.7217165231704712), (0.5405843257904053, 0.7886179089546204), (0.5405843257904053, 0.9309148192405701), (0.5405843257904053, 0.8690909147262573), (0.5405843257904053, 0.8487862944602966), (0.5405843257904053, 0.8441049456596375), (0.5405843257904053, 0.8233137726783752), (0.5405843257904053, 0.8329488039016724), (0.5405843257904053, 0.7996525764465332), (0.5405843257904053, 0.6938588619232178), (0.5405843257904053, 0.7153162956237793), (0.5405843257904053, 0.7635959982872009), (0.5405843257904053, 0.8163197636604309), (0.5405843257904053, 0.8794442415237427), (0.5405843257904053, 0.933911144733429), (0.5405843257904053, 0.8887931108474731), (0.5405843257904053, 0.8699421882629395), (0.5405843257904053, 0.8794977068901062), (0.5405843257904053, 0.9074000716209412), (0.5405843257904053, 0.9519230723381042), (0.5405843257904053, 0.8827101588249207), (0.5405843257904053, 0.6930122375488281), (0.5405843257904053, 0.8020689487457275), (0.5405843257904053, 0.8164047598838806), (0.5405843257904053, 0.8765270709991455), (0.5405843257904053, 0.9049001932144165), (0.5405843257904053, 0.8807123899459839), (0.5405843257904053, 0.8715388178825378), (0.5405843257904053, 0.8674256205558777), (0.5405843257904053, 0.8541341423988342), (0.5405843257904053, 0.8096724152565002), (0.5405843257904053, 0.9355570077896118), (0.5405843257904053, 0.9491460919380188), (0.5405843257904053, 0.7939581871032715), (0.5405843257904053, 0.7215337157249451), (0.5405843257904053, 0.8340662717819214), (0.5405843257904053, 0.8740275502204895), (0.5405843257904053, 0.9538661241531372), (0.5405843257904053, 0.897335946559906), (0.5405843257904053, 0.871188759803772), (0.5405843257904053, 0.8856115341186523), (0.5405843257904053, 0.8700637221336365), (0.5405843257904053, 0.8023537397384644), (0.5405843257904053, 0.9579756259918213), (0.5405843257904053, 0.8450998067855835), (0.5405843257904053, 0.7935315370559692), (0.5405843257904053, 0.8494572639465332), (0.5405843257904053, 0.8832487463951111), (0.5405843257904053, 0.9685373902320862), (0.5405843257904053, 0.9128055572509766), (0.5405843257904053, 0.905209481716156), (0.5405843257904053, 0.957156777381897), (0.5405843257904053, 0.8061674237251282), (0.5405843257904053, 0.8403387665748596), (0.5405843257904053, 0.8755035996437073), (0.5405843257904053, 0.9637588262557983), (0.5405843257904053, 0.9089854955673218), (0.5405843257904053, 0.9051763415336609), (0.5405843257904053, 0.8954529166221619), (0.5405843257904053, 0.9166401624679565), (0.5405843257904053, 0.9617224931716919), (0.5405843257904053, 0.8306780457496643), (0.5405843257904053, 0.7061885595321655), (0.5405843257904053, 0.7927621603012085), (0.5405843257904053, 0.8921271562576294), (0.5405843257904053, 0.9726402163505554), (0.5405843257904053, 0.9668908715248108), (0.5405843257904053, 0.8904255032539368)]\n",
      "    1 |    16.5914 |    0.5449 |   1.0000 |    1.0000 *\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: 2\n",
      "Target Params: 500000\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 1.0, Params: -425183\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 2048\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 4.0, Params: 1170529\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.5, Params: -251551\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 768\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 3.25, Params: 161889\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.875, Params: -251551\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 768\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 3.0625, Params: 161889\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.96875, Params: -251551\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 768\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 3.015625, Params: 161889\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.9921875, Params: -251551\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 768\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 3.00390625, Params: 161889\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.998046875, Params: -251551\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 768\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 3.0009765625, Params: 161889\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.99951171875, Params: -251551\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 768\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 3.000244140625, Params: 161889\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.9998779296875, Params: -251551\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 768\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 3.00006103515625, Params: 161889\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.999969482421875, Params: -251551\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 768\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 3.0000152587890625, Params: 161889\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.9999923706054688, Params: -251551\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 768\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 3.0000038146972656, Params: 161889\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.999998092651367, Params: -251551\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 768\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 3.0000009536743164, Params: 161889\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.999999523162842, Params: -251551\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 768\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 3.000000238418579, Params: 161889\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.9999998807907104, Params: -251551\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 768\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 3.0000000596046448, Params: 161889\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.9999999701976776, Params: -251551\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 768\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 3.000000014901161, Params: 161889\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.9999999925494194, Params: -251551\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 768\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 3.0000000037252903, Params: 161889\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.999999998137355, Params: -251551\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 768\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 3.0000000009313226, Params: 161889\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.9999999995343387, Params: -251551\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 768\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 3.0000000002328306, Params: 161889\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.9999999998835847, Params: -251551\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 768\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 3.0000000000582077, Params: 161889\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.999999999970896, Params: -251551\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 768\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 3.000000000014552, Params: 161889\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.999999999992724, Params: -251551\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 768\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 3.000000000003638, Params: 161889\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.999999999998181, Params: -251551\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 768\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 3.0000000000009095, Params: 161889\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 2.9999999999995453, Params: -251551\n",
      "(1) Optimal h: 2\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 256\n",
      "DEBUG: prev_out_dim = 128\n",
      "Model Summary:\n",
      "--------------------------------------------------\n",
      "Model Type: DGCNN\n",
      "Number of Layers: 2\n",
      "Number of Parameters: 248449\n",
      "Device: mps\n",
      "--------------------------------------------------\n",
      "Layer Details:\n",
      "ModuleList(\n",
      "  (0): EdgeConv(nn=Sequential(\n",
      "    (0): Linear(in_features=6, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "  ))\n",
      "  (1): EdgeConv(nn=Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "  ))\n",
      ")\n",
      "Epoch | Train Loss |  Val Loss |  Val Acc |  Test Acc\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5494f045de4a46959df960740ecb41fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on  mps:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93fb709ac7346b494555d51fb5933ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[-0.0186],\n",
      "        [-0.0184],\n",
      "        [-0.0185],\n",
      "        [-0.0185],\n",
      "        [-0.0181],\n",
      "        [-0.0184],\n",
      "        [-0.0185],\n",
      "        [-0.0187],\n",
      "        [-0.0184],\n",
      "        [-0.0183],\n",
      "        [-0.0182],\n",
      "        [-0.0189],\n",
      "        [-0.0182],\n",
      "        [-0.0182],\n",
      "        [-0.0185],\n",
      "        [-0.0183],\n",
      "        [-0.0185],\n",
      "        [-0.0183],\n",
      "        [-0.0187],\n",
      "        [-0.0186],\n",
      "        [-0.0184],\n",
      "        [-0.0179],\n",
      "        [-0.0182],\n",
      "        [-0.0185],\n",
      "        [-0.0184],\n",
      "        [-0.0186],\n",
      "        [-0.0183],\n",
      "        [-0.0183],\n",
      "        [-0.0184],\n",
      "        [-0.0181],\n",
      "        [-0.0184],\n",
      "        [-0.0186]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002],\n",
      "        [0.0002]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062],\n",
      "        [0.0062]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123],\n",
      "        [0.0123]], device='mps:0', grad_fn=<LinearBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 30.54 GB, other allocations: 5.24 GB, max allowed: 36.27 GB). Tried to allocate 512.94 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 89\u001b[39m\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available():\n\u001b[32m     87\u001b[39m         torch.cuda.empty_cache()\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m                    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m                    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarget_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m100_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m500_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10_000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m                     \u001b[38;5;66;03m# for target_params in tqdm([1_000_000, 100_000, 500_000, 10_000]))\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/joblib/parallel.py:1918\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1916\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1917\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[32m   1920\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1921\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1922\u001b[39m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1923\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1924\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/joblib/parallel.py:1847\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1847\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1849\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mf\u001b[39m\u001b[34m(n_layers, target_params)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m trange(MAX_EPOCHS):\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[32m     49\u001b[39m     preds_val, actuals_val, acc_val, val_loss = evaluate(model,loader_val)\n\u001b[32m     50\u001b[39m     preds_test, actuals_test, acc_test, test_loss = evaluate(model,loader_test)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, loader_train, optimizer)\u001b[39m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(batch.y.device).startswith(\u001b[38;5;28mstr\u001b[39m(device)), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbatch.y on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch.y.device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     58\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m out = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m                \u001b[38;5;66;03m# [batch_size,1]\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# graph-level target\u001b[39;00m\n\u001b[32m     61\u001b[39m y_nodes = batch.y.float().unsqueeze(-\u001b[32m1\u001b[39m)     \u001b[38;5;66;03m# [num_nodes,1]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 150\u001b[39m, in \u001b[36mDGCNN.forward\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    147\u001b[39m x, edge_index, batch = data.x.to(\u001b[38;5;28mself\u001b[39m.device), data.edge_index.to(\u001b[38;5;28mself\u001b[39m.device), data.batch.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[:\u001b[38;5;28mself\u001b[39m.num_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     x = \u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# Global mean pooling\u001b[39;00m\n\u001b[32m    153\u001b[39m x_pool = global_mean_pool(x, batch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/torch_geometric/nn/conv/edge_conv.py:62\u001b[39m, in \u001b[36mEdgeConv.forward\u001b[39m\u001b[34m(self, x, edge_index)\u001b[39m\n\u001b[32m     59\u001b[39m     x = (x, x)\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# propagate_type: (x: PairTensor)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/vp/k8zj_r85147blp63h74mbprr0000gn/T/torch_geometric.nn.conv.edge_conv_EdgeConv_propagate_gy7wrxvm.py:187\u001b[39m, in \u001b[36mpropagate\u001b[39m\u001b[34m(self, edge_index, x, size)\u001b[39m\n\u001b[32m    178\u001b[39m             kwargs = CollectArgs(\n\u001b[32m    179\u001b[39m                 x_i=hook_kwargs[\u001b[33m'\u001b[39m\u001b[33mx_i\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    180\u001b[39m                 x_j=hook_kwargs[\u001b[33m'\u001b[39m\u001b[33mx_j\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    183\u001b[39m                 dim_size=kwargs.dim_size,\n\u001b[32m    184\u001b[39m             )\n\u001b[32m    185\u001b[39m \u001b[38;5;66;03m# End Message Forward Pre Hook #########################################\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx_i\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mx_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx_j\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mx_j\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# Begin Message Forward Hook ###########################################\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.jit.is_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/torch_geometric/nn/conv/edge_conv.py:65\u001b[39m, in \u001b[36mEdgeConv.message\u001b[39m\u001b[34m(self, x_i, x_j)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmessage\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_i: Tensor, x_j: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_j\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_i\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: MPS backend out of memory (MPS allocated: 30.54 GB, other allocations: 5.24 GB, max allowed: 36.27 GB). Tried to allocate 512.94 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "def print_model_summary(model):\n",
    "    print(\"Model Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Model Type: {type(model).__name__}\")\n",
    "    print(f\"Number of Layers: {model.num_layers}\")\n",
    "    print(f\"Number of Parameters: {count_parameters(model)}\")\n",
    "    print(f\"Device: {model.device}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Layer Details:\")\n",
    "    print(model.layers)\n",
    "\n",
    "nom_args = (3, 4, 1)\n",
    "\n",
    "def f(n_layers, target_params):\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(f\"Layers: { n_layers }\")\n",
    "    print(f\"Target Params: { target_params }\")\n",
    "    # Find out the hyperparameteres yielding #params = target_params\n",
    "    def objective(h):\n",
    "        out = count_parameters(DGCNN(*nom_args, hidden_mult=int(h), num_layers=n_layers, device=device)) - target_params\n",
    "        print(f\"Hidden Mult: {h}, Params: {out}\")\n",
    "        return out\n",
    "    \n",
    "    optimal_h = int(root_scalar(objective, bracket=[1, 4], method='bisect').root)\n",
    "    print(f\"(1) Optimal h: {optimal_h}\")\n",
    "    # optimal_h= pd.Series({optimal_h:target_params-count_parameters(DGCNN(*nom_args, hidden_mult=optimal_h, num_layers=n_layers)),\n",
    "    #             optimal_h-1:target_params-count_parameters(DGCNN(*nom_args, hidden_mult=optimal_h-1, num_layers=n_layers)),\n",
    "    #             optimal_h+1:target_params-count_parameters(DGCNN(*nom_args, hidden_mult=optimal_h+1, num_layers=n_layers))}).abs().idxmin()\n",
    "    # print(f\"(2) Optimal h: {optimal_h}\")\n",
    "    \n",
    "    # model = DGCNN(optimal_h,n_layers).to(device) \n",
    "    # model = InteractionNetwork(optimal_h,n_layers).to(device)\n",
    "    model = DGCNN(*nom_args, hidden_mult=optimal_h, num_layers=n_layers, device=device)\n",
    "    print_model_summary(model)\n",
    "    lr = LR\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve, epochs_no_improve2 = 0,0\n",
    "    best_model_state = None\n",
    "    stats = []\n",
    "    best = None\n",
    "    # Print header once\n",
    "    print(f\"{'Epoch':>5} | {'Train Loss':>10} | {'Val Loss':>9} | {'Val Acc':>8} | {'Test Acc':>9}\")\n",
    "    print(\"-\" * 50)\n",
    "    for epoch in trange(MAX_EPOCHS):\n",
    "        train_loss = train_epoch(model, loader_train, optimizer)   \n",
    "        preds_val, actuals_val, acc_val, val_loss = evaluate(model,loader_val)\n",
    "        preds_test, actuals_test, acc_test, test_loss = evaluate(model,loader_test)\n",
    "        \n",
    "        stats.append({'train_loss':train_loss, 'val_loss':val_loss, 'acc_val':acc_val, 'acc_test':acc_test})\n",
    "        if val_loss < best_val_loss: \n",
    "            print(f\"{epoch+1:5d} | {train_loss:10.4f} | {val_loss:9.4f} | {acc_val:8.4f} | {acc_test:9.4f} *\")\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            epochs_no_improve2 = 0\n",
    "            best = {'model_state': {k: v.cpu() for k, v in model.state_dict().items()},\n",
    "                    'preds_test':preds_test, 'preds_val':preds_val}        \n",
    "        else:\n",
    "            print(f\"{epoch+1:5d} | {train_loss:10.4f} | {val_loss:9.4f} | {acc_val:8.4f} | {acc_test:9.4f}\")\n",
    "            epochs_no_improve += 1\n",
    "            epochs_no_improve2 += 1\n",
    "\n",
    "        if epochs_no_improve >= TOLERANCE:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        if epochs_no_improve2 >= LR_TOLERANCE:\n",
    "            if lr >=1.0e-8:\n",
    "                lr/=10\n",
    "            print(f\"LR reduction to {lr}\")\n",
    "    best['stats'] = stats\n",
    "    os.makedirs(PATH_DATA, exist_ok=True)\n",
    "    joblib.dump(best, os.path.join(PATH_DATA, f\"L{n_layers}.pkl\"))\n",
    "    \n",
    "    stats = pd.DataFrame(stats)\n",
    "    stats[['train_loss','val_loss']].plot(figsize = (15,4))\n",
    "    plt.show()\n",
    "    stats[['acc_val','acc_test']].plot(figsize = (15,4))\n",
    "    plt.show()\n",
    "    del model\n",
    "    del train_loss\n",
    "    del optimizer\n",
    "\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "Parallel(n_jobs=1)(delayed(f)(n_layers, target_params)\n",
    "                    for n_layers in tqdm([2,3])\n",
    "                    for target_params in tqdm([100_000, 500_000, 10_000]))\n",
    "                    # for target_params in tqdm([1_000_000, 100_000, 500_000, 10_000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7e8cf251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Params: 74817\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers == 1\n",
      "DEBUG: edgeconv_out_dim = 128\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 1, Params: 24409\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Params: 74817\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers == 1\n",
      "DEBUG: edgeconv_out_dim = 128\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 1, Params: 24409\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Params: 74817\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers == 1\n",
      "DEBUG: edgeconv_out_dim = 128\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 1, Params: 24409\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Params: 74817\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers == 1\n",
      "DEBUG: edgeconv_out_dim = 128\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 1, Params: 24409\n",
      "Hidden Mult: 1, Params: 24409\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n",
      "DEBUG: i = 2\n",
      "last layer\n",
      "DEBUG: edgeconv_out_dim = 64\n",
      "DEBUG: prev_out_dim = 128\n",
      "Params: 74817\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers == 1\n",
      "DEBUG: edgeconv_out_dim = 128\n",
      "DEBUG: prev_out_dim = 128\n",
      "Hidden Mult: 1.0, Params: 24409\n",
      "DEBUG: i = 1\n",
      "first layer, num_layers > 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, torch.memory_format memory_format = None, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHidden Mult: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_h\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobjective(_h)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHidden Mult: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_h\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobjective(_h)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m optimal_h = \u001b[43mroot_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbracket\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbisect\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m.root\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/scipy/optimize/_root_scalar.py:286\u001b[39m, in \u001b[36mroot_scalar\u001b[39m\u001b[34m(f, args, method, bracket, fprime, fprime2, x0, x1, xtol, rtol, maxiter, options)\u001b[39m\n\u001b[32m    284\u001b[39m a, b = bracket[:\u001b[32m2\u001b[39m]\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     r, sol = \u001b[43mmethodc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    288\u001b[39m     \u001b[38;5;66;03m# gh-17622 fixed some bugs in low-level solvers by raising an error\u001b[39;00m\n\u001b[32m    289\u001b[39m     \u001b[38;5;66;03m# (rather than returning incorrect results) when the callable\u001b[39;00m\n\u001b[32m    290\u001b[39m     \u001b[38;5;66;03m# returns a NaN. It did so by wrapping the callable rather than\u001b[39;00m\n\u001b[32m    291\u001b[39m     \u001b[38;5;66;03m# modifying compiled code, so the iteration count is not available.\u001b[39;00m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33m_x\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/scipy/optimize/_zeros_py.py:577\u001b[39m, in \u001b[36mbisect\u001b[39m\u001b[34m(f, a, b, args, xtol, rtol, maxiter, full_output, disp)\u001b[39m\n\u001b[32m    575\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mrtol too small (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrtol\u001b[38;5;132;01m:\u001b[39;00m\u001b[33mg\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m < \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_rtol\u001b[38;5;132;01m:\u001b[39;00m\u001b[33mg\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    576\u001b[39m f = _wrap_nan_raise(f)\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m r = \u001b[43m_zeros\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_bisect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results_c(full_output, r, \u001b[33m\"\u001b[39m\u001b[33mbisect\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/scipy/optimize/_zeros_py.py:94\u001b[39m, in \u001b[36m_wrap_nan_raise.<locals>.f_raise\u001b[39m\u001b[34m(x, *args)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mf_raise\u001b[39m(x, *args):\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     fx = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m     f_raise._function_calls += \u001b[32m1\u001b[39m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m np.isnan(fx):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(h)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mobjective\u001b[39m(h):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mParams: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount_parameters(\u001b[43mDGCNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mnom_args\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mhidden_mult\u001b[49m\u001b[43m=\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m     out = count_parameters(DGCNN(*nom_args, hidden_mult=h, num_layers=\u001b[32m1\u001b[39m, device=device)) - \u001b[32m1_000\u001b[39m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHidden Mult: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mDGCNN.__init__\u001b[39m\u001b[34m(self, num_node_features, num_edge_features, num_classes, num_layers, hidden_mult, device)\u001b[39m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mfirst layer, num_layers > 1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     44\u001b[39m     next_out_dim = \u001b[38;5;28mself\u001b[39m._ndim_map(i + \u001b[32m1\u001b[39m)\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mself\u001b[39m.layers.append(\n\u001b[32m     46\u001b[39m         pyg.nn.EdgeConv(nn.Sequential(\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m             \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_node_features\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_out_dim\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     48\u001b[39m             nn.ReLU(),\n\u001b[32m     49\u001b[39m             nn.Linear(prev_out_dim, next_out_dim),\n\u001b[32m     50\u001b[39m             nn.ReLU()\n\u001b[32m     51\u001b[39m         ))\n\u001b[32m     52\u001b[39m     )\n\u001b[32m     53\u001b[39m     prev_out_dim = next_out_dim\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m i == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m num_layers == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/torch/nn/modules/linear.py:106\u001b[39m, in \u001b[36mLinear.__init__\u001b[39m\u001b[34m(self, in_features, out_features, bias, device, dtype)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28mself\u001b[39m.in_features = in_features\n\u001b[32m    104\u001b[39m \u001b[38;5;28mself\u001b[39m.out_features = out_features\n\u001b[32m    105\u001b[39m \u001b[38;5;28mself\u001b[39m.weight = Parameter(\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m )\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[32m    109\u001b[39m     \u001b[38;5;28mself\u001b[39m.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
      "\u001b[31mTypeError\u001b[39m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, torch.memory_format memory_format = None, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n"
     ]
    }
   ],
   "source": [
    "def objective(h):\n",
    "    print(f\"Params: {count_parameters(DGCNN(*nom_args, hidden_mult=h, num_layers=2, device=device))}\")\n",
    "    out = count_parameters(DGCNN(*nom_args, hidden_mult=h, num_layers=1, device=device)) - 1_000\n",
    "    print(f\"Hidden Mult: {h}, Params: {out}\")\n",
    "    return out\n",
    "\n",
    "\n",
    "_h = 1 # init\n",
    "step = 0.1\n",
    "if objective(_h) < 0 and _h >= 1:\n",
    "    while objective(_h) > 0:\n",
    "        _h -= step if _h > 1 else 0\n",
    "        print(f\"Hidden Mult: {_h}, Params: {objective(_h)}\")\n",
    "elif objective(_h) > 0:\n",
    "    while objective(_h) < 0:\n",
    "        _h += step\n",
    "        print(f\"Hidden Mult: {_h}, Params: {objective(_h)}\")\n",
    "print(f\"Hidden Mult: {_h}, Params: {objective(_h)}\")\n",
    "\n",
    "optimal_h = root_scalar(objective, bracket=[1, 10], method='bisect').root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "de2ee8b9-8fb9-46cd-832a-25d4d9b091a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a9121eb19a14c9e9734a31edd9e980b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ffcaecb91264c138c5cd05590d84e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available():\n\u001b[32m     66\u001b[39m         torch.cuda.empty_cache()\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m                    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m                    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarget_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1_000_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m100_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m                                               \u001b[49m\u001b[32;43m500_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/joblib/parallel.py:1918\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1916\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1917\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1921\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1922\u001b[39m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1923\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1924\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/joblib/parallel.py:1847\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1847\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1849\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mf\u001b[39m\u001b[34m(n_layers, target_params)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mobjective\u001b[39m(h):\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m count_parameters(InteractionNetwork(\u001b[38;5;28mint\u001b[39m(h),n_layers)) - target_params\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m optimal_h = \u001b[38;5;28mint\u001b[39m(\u001b[43mroot_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbracket\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbisect\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m.root)\n\u001b[32m     10\u001b[39m optimal_h= pd.Series({optimal_h:target_params-count_parameters(InteractionNetwork(optimal_h, n_layers)),\n\u001b[32m     11\u001b[39m             optimal_h-\u001b[32m1\u001b[39m:target_params-count_parameters(InteractionNetwork(optimal_h-\u001b[32m1\u001b[39m,n_layers)),\n\u001b[32m     12\u001b[39m             optimal_h+\u001b[32m1\u001b[39m:target_params-count_parameters(InteractionNetwork(optimal_h+\u001b[32m1\u001b[39m,n_layers))}).abs().idxmin()\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# model = InteractionNetwork(optimal_h,n_layers).to(device)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/scipy/optimize/_root_scalar.py:286\u001b[39m, in \u001b[36mroot_scalar\u001b[39m\u001b[34m(f, args, method, bracket, fprime, fprime2, x0, x1, xtol, rtol, maxiter, options)\u001b[39m\n\u001b[32m    284\u001b[39m a, b = bracket[:\u001b[32m2\u001b[39m]\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     r, sol = \u001b[43mmethodc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    288\u001b[39m     \u001b[38;5;66;03m# gh-17622 fixed some bugs in low-level solvers by raising an error\u001b[39;00m\n\u001b[32m    289\u001b[39m     \u001b[38;5;66;03m# (rather than returning incorrect results) when the callable\u001b[39;00m\n\u001b[32m    290\u001b[39m     \u001b[38;5;66;03m# returns a NaN. It did so by wrapping the callable rather than\u001b[39;00m\n\u001b[32m    291\u001b[39m     \u001b[38;5;66;03m# modifying compiled code, so the iteration count is not available.\u001b[39;00m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33m_x\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/scipy/optimize/_zeros_py.py:577\u001b[39m, in \u001b[36mbisect\u001b[39m\u001b[34m(f, a, b, args, xtol, rtol, maxiter, full_output, disp)\u001b[39m\n\u001b[32m    575\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mrtol too small (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrtol\u001b[38;5;132;01m:\u001b[39;00m\u001b[33mg\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m < \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_rtol\u001b[38;5;132;01m:\u001b[39;00m\u001b[33mg\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    576\u001b[39m f = _wrap_nan_raise(f)\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m r = \u001b[43m_zeros\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_bisect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results_c(full_output, r, \u001b[33m\"\u001b[39m\u001b[33mbisect\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/scipy/optimize/_zeros_py.py:94\u001b[39m, in \u001b[36m_wrap_nan_raise.<locals>.f_raise\u001b[39m\u001b[34m(x, *args)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mf_raise\u001b[39m(x, *args):\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     fx = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m     f_raise._function_calls += \u001b[32m1\u001b[39m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m np.isnan(fx):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mf.<locals>.objective\u001b[39m\u001b[34m(h)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mobjective\u001b[39m(h):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m count_parameters(\u001b[43mInteractionNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m)\u001b[49m) - target_params\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mInteractionNetwork.__init__\u001b[39m\u001b[34m(self, hidden_size, n_layers)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_size, n_layers):\n\u001b[32m     37\u001b[39m     \u001b[38;5;28msuper\u001b[39m(InteractionNetwork, \u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m(aggr=\u001b[33m'\u001b[39m\u001b[33madd\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     38\u001b[39m                                              flow=\u001b[33m'\u001b[39m\u001b[33msource_to_target\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28mself\u001b[39m.R1 = \u001b[43mRelationalModel\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28mself\u001b[39m.O = ObjectModel(\u001b[32m7\u001b[39m, \u001b[32m3\u001b[39m, hidden_size, n_layers)\n\u001b[32m     41\u001b[39m     \u001b[38;5;28mself\u001b[39m.R2 = RelationalModel(\u001b[32m10\u001b[39m, \u001b[32m1\u001b[39m, hidden_size, n_layers)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mRelationalModel.__init__\u001b[39m\u001b[34m(self, input_size, output_size, hidden_size, n_layers)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_layers>=\u001b[32m2\u001b[39m:\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_layers - \u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m         layers.append(\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     10\u001b[39m         layers.append(nn.ReLU())\n\u001b[32m     12\u001b[39m layers.append(nn.Linear(hidden_size, output_size))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/torch/nn/modules/linear.py:112\u001b[39m, in \u001b[36mLinear.__init__\u001b[39m\u001b[34m(self, in_features, out_features, bias, device, dtype)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    111\u001b[39m     \u001b[38;5;28mself\u001b[39m.register_parameter(\u001b[33m\"\u001b[39m\u001b[33mbias\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/torch/nn/modules/linear.py:118\u001b[39m, in \u001b[36mLinear.reset_parameters\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    115\u001b[39m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[32m    116\u001b[39m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[32m    117\u001b[39m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     \u001b[43minit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmath\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    120\u001b[39m         fan_in, _ = init._calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m.weight)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/gnn-class/lib/python3.11/site-packages/torch/nn/init.py:518\u001b[39m, in \u001b[36mkaiming_uniform_\u001b[39m\u001b[34m(tensor, a, mode, nonlinearity, generator)\u001b[39m\n\u001b[32m    516\u001b[39m bound = math.sqrt(\u001b[32m3.0\u001b[39m) * std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[32m    517\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m518\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def f(n_layers,target_params):\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(n_layers, target_params)\n",
    "    # Find out the hyperparameteres yielding #params = target_params\n",
    "    def objective(h):\n",
    "        return count_parameters(InteractionNetwork(int(h),n_layers)) - target_params\n",
    "    \n",
    "    optimal_h = int(root_scalar(objective, bracket=[1, 50000], method='bisect').root)\n",
    "    print(f\"(1) Optimal h: {optimal_h}\")\n",
    "\n",
    "    optimal_h= pd.Series({optimal_h:target_params-count_parameters(InteractionNetwork(optimal_h, n_layers)),\n",
    "                optimal_h-1:target_params-count_parameters(InteractionNetwork(optimal_h-1,n_layers)),\n",
    "                optimal_h+1:target_params-count_parameters(InteractionNetwork(optimal_h+1,n_layers))}).abs().idxmin()\n",
    "    print(f\"(2) Optimal h: {optimal_h}\")\n",
    "    \n",
    "    # model = InteractionNetwork(optimal_h,n_layers).to(device)\n",
    "    model = DGCNN(3, 4, 1).to(device)\n",
    "    lr = LR\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve, epochs_no_improve2 = 0,0\n",
    "    best_model_state = None\n",
    "    stats = []\n",
    "    best = None\n",
    "    # Print header once\n",
    "    print(f\"{'Epoch':>5} | {'Train Loss':>10} | {'Val Loss':>9} | {'Val Acc':>8} | {'Test Acc':>9}\")\n",
    "    print(\"-\" * 50)\n",
    "    for epoch in trange(MAX_EPOCHS):\n",
    "        train_loss = train_epoch(model, loader_train, optimizer)   \n",
    "        preds_val, actuals_val, acc_val, val_loss = evaluate(model,loader_val)\n",
    "        preds_test, actuals_test, acc_test, test_loss = evaluate(model,loader_test)\n",
    "        \n",
    "        stats.append({'train_loss':train_loss, 'val_loss':val_loss, 'acc_val':acc_val, 'acc_test':acc_test})\n",
    "        if val_loss < best_val_loss: \n",
    "            print(f\"{epoch+1:5d} | {train_loss:10.4f} | {val_loss:9.4f} | {acc_val:8.4f} | {acc_test:9.4f} *\")\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            epochs_no_improve2 = 0\n",
    "            best = {'model_state': {k: v.cpu() for k, v in model.state_dict().items()},\n",
    "                    'preds_test':preds_test, 'preds_val':preds_val}        \n",
    "        else:\n",
    "            print(f\"{epoch+1:5d} | {train_loss:10.4f} | {val_loss:9.4f} | {acc_val:8.4f} | {acc_test:9.4f}\")\n",
    "            epochs_no_improve += 1\n",
    "            epochs_no_improve2 += 1\n",
    "\n",
    "        if epochs_no_improve >= TOLERANCE:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        if epochs_no_improve2 >= LR_TOLERANCE:\n",
    "            if lr >=1.0e-8:\n",
    "                lr/=10\n",
    "            print(f\"LR reduction to {lr}\")\n",
    "    best['stats'] = stats\n",
    "    os.makedirs(PATH_DATA, exist_ok=True)\n",
    "    joblib.dump(best, os.path.join(PATH_DATA, f\"{n_layers}_{target_params}.pkl\"))\n",
    "    \n",
    "    stats = pd.DataFrame(stats)\n",
    "    stats[['train_loss','val_loss']].plot(figsize = (15,4))\n",
    "    plt.show()\n",
    "    stats[['acc_val','acc_test']].plot(figsize = (15,4))\n",
    "    plt.show()\n",
    "    del model\n",
    "    del train_loss\n",
    "    del optimizer\n",
    "\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "Parallel(n_jobs=1)(delayed(f)(n_layers,target_params)\n",
    "                    for n_layers in tqdm([2,3,4])\n",
    "                    for target_params in tqdm([1_000_000,100_000,\n",
    "                                               500_000,]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e401f1-823e-4422-8908-775d77c471f8",
   "metadata": {},
   "source": [
    "# Summary of the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ab51456-0db4-4a57-b2f9-e5e37bb9d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.read_pickle(os.path.join(PATH_DATA0, 'graphs','max_prob_10_subsample_0.1','graphs_val.pkl'))\n",
    "test = pd.read_pickle(os.path.join(PATH_DATA0, 'graphs','max_prob_10_subsample_0.1','graphs_test.pkl'))\n",
    "val_y = torch.cat([i.y for i in val.values]).numpy()\n",
    "test_y = torch.cat([i.y for i in test.values]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9534902e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.3406],\n",
      "        [0.3448],\n",
      "        [0.3389],\n",
      "        [0.3434],\n",
      "        [0.3434],\n",
      "        [0.3406],\n",
      "        [0.3394],\n",
      "        [0.3444],\n",
      "        [0.3404],\n",
      "        [0.3401],\n",
      "        [0.3415],\n",
      "        [0.3432],\n",
      "        [0.3387],\n",
      "        [0.3406],\n",
      "        [0.3458],\n",
      "        [0.3404],\n",
      "        [0.3403],\n",
      "        [0.3433],\n",
      "        [0.3371],\n",
      "        [0.3428],\n",
      "        [0.3411],\n",
      "        [0.3404],\n",
      "        [0.3416],\n",
      "        [0.3416],\n",
      "        [0.3391],\n",
      "        [0.3419],\n",
      "        [0.3398],\n",
      "        [0.3416],\n",
      "        [0.3429],\n",
      "        [0.3381],\n",
      "        [0.3400],\n",
      "        [0.3418]], device='mps:0', grad_fn=<LinearBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "val0 = next(iter(loader_val))\n",
    "preds = model(val0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cedd0134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1044834, 1])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val0.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "acdc9319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.3429],\n",
      "        [0.3392],\n",
      "        [0.3415],\n",
      "        [0.3425],\n",
      "        [0.3396],\n",
      "        [0.3408],\n",
      "        [0.3416],\n",
      "        [0.3416],\n",
      "        [0.3435],\n",
      "        [0.3417],\n",
      "        [0.3419],\n",
      "        [0.3453],\n",
      "        [0.3422],\n",
      "        [0.3422],\n",
      "        [0.3385],\n",
      "        [0.3410],\n",
      "        [0.3353],\n",
      "        [0.3396],\n",
      "        [0.3411],\n",
      "        [0.3391],\n",
      "        [0.3394],\n",
      "        [0.3378],\n",
      "        [0.3404],\n",
      "        [0.3425],\n",
      "        [0.3424],\n",
      "        [0.3457],\n",
      "        [0.3419],\n",
      "        [0.3394],\n",
      "        [0.3400],\n",
      "        [0.3399],\n",
      "        [0.3399],\n",
      "        [0.3360]], device='mps:0')\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.3424],\n",
      "        [0.3379],\n",
      "        [0.3379],\n",
      "        [0.3374],\n",
      "        [0.3404],\n",
      "        [0.3425],\n",
      "        [0.3387],\n",
      "        [0.3406],\n",
      "        [0.3386],\n",
      "        [0.3364],\n",
      "        [0.3426],\n",
      "        [0.3398],\n",
      "        [0.3378],\n",
      "        [0.3405],\n",
      "        [0.3405],\n",
      "        [0.3398],\n",
      "        [0.3415],\n",
      "        [0.3414],\n",
      "        [0.3391],\n",
      "        [0.3414],\n",
      "        [0.3414],\n",
      "        [0.3388],\n",
      "        [0.3509],\n",
      "        [0.3391],\n",
      "        [0.3480],\n",
      "        [0.3448],\n",
      "        [0.3428],\n",
      "        [0.3403],\n",
      "        [0.3393],\n",
      "        [0.3399],\n",
      "        [0.3384],\n",
      "        [0.3421]], device='mps:0')\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.3402],\n",
      "        [0.3457],\n",
      "        [0.3410],\n",
      "        [0.3462],\n",
      "        [0.3419],\n",
      "        [0.3404],\n",
      "        [0.3385],\n",
      "        [0.3432],\n",
      "        [0.3422],\n",
      "        [0.3432],\n",
      "        [0.3388],\n",
      "        [0.3390],\n",
      "        [0.3372],\n",
      "        [0.3415],\n",
      "        [0.3430],\n",
      "        [0.3430],\n",
      "        [0.3401],\n",
      "        [0.3413],\n",
      "        [0.3419],\n",
      "        [0.3398],\n",
      "        [0.3400],\n",
      "        [0.3419],\n",
      "        [0.3391],\n",
      "        [0.3404],\n",
      "        [0.3444],\n",
      "        [0.3452],\n",
      "        [0.3432],\n",
      "        [0.3452],\n",
      "        [0.3385],\n",
      "        [0.3439],\n",
      "        [0.3438],\n",
      "        [0.3414]], device='mps:0')\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.3426],\n",
      "        [0.3455],\n",
      "        [0.3412],\n",
      "        [0.3372],\n",
      "        [0.3406],\n",
      "        [0.3410],\n",
      "        [0.3417],\n",
      "        [0.3415],\n",
      "        [0.3388],\n",
      "        [0.3373],\n",
      "        [0.3406],\n",
      "        [0.3389],\n",
      "        [0.3359],\n",
      "        [0.3424],\n",
      "        [0.3434],\n",
      "        [0.3429],\n",
      "        [0.3396],\n",
      "        [0.3402],\n",
      "        [0.3433],\n",
      "        [0.3385],\n",
      "        [0.3427],\n",
      "        [0.3428],\n",
      "        [0.3383],\n",
      "        [0.3397],\n",
      "        [0.3397],\n",
      "        [0.3377],\n",
      "        [0.3391],\n",
      "        [0.3411],\n",
      "        [0.3398],\n",
      "        [0.3427],\n",
      "        [0.3415],\n",
      "        [0.3385]], device='mps:0')\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.3395],\n",
      "        [0.3378],\n",
      "        [0.3388],\n",
      "        [0.3405],\n",
      "        [0.3404],\n",
      "        [0.3405],\n",
      "        [0.3415],\n",
      "        [0.3413],\n",
      "        [0.3384],\n",
      "        [0.3393],\n",
      "        [0.3404],\n",
      "        [0.3404],\n",
      "        [0.3381],\n",
      "        [0.3404],\n",
      "        [0.3437],\n",
      "        [0.3437],\n",
      "        [0.3438],\n",
      "        [0.3410],\n",
      "        [0.3427],\n",
      "        [0.3427],\n",
      "        [0.3390],\n",
      "        [0.3430],\n",
      "        [0.3402],\n",
      "        [0.3451],\n",
      "        [0.3436],\n",
      "        [0.3362],\n",
      "        [0.3412],\n",
      "        [0.3476],\n",
      "        [0.3396],\n",
      "        [0.3407],\n",
      "        [0.3426],\n",
      "        [0.3428]], device='mps:0')\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([17, 128])\n",
      "DEBUG: x_out shape = torch.Size([17, 1])\n",
      "tensor([[0.3358],\n",
      "        [0.3408],\n",
      "        [0.3406],\n",
      "        [0.3426],\n",
      "        [0.3413],\n",
      "        [0.3420],\n",
      "        [0.3491],\n",
      "        [0.3435],\n",
      "        [0.3417],\n",
      "        [0.3404],\n",
      "        [0.3388],\n",
      "        [0.3390],\n",
      "        [0.3397],\n",
      "        [0.3395],\n",
      "        [0.3354],\n",
      "        [0.3430],\n",
      "        [0.3490]], device='mps:0')\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.3406],\n",
      "        [0.3448],\n",
      "        [0.3389],\n",
      "        [0.3434],\n",
      "        [0.3434],\n",
      "        [0.3406],\n",
      "        [0.3394],\n",
      "        [0.3444],\n",
      "        [0.3404],\n",
      "        [0.3401],\n",
      "        [0.3415],\n",
      "        [0.3432],\n",
      "        [0.3387],\n",
      "        [0.3406],\n",
      "        [0.3458],\n",
      "        [0.3404],\n",
      "        [0.3403],\n",
      "        [0.3433],\n",
      "        [0.3371],\n",
      "        [0.3428],\n",
      "        [0.3411],\n",
      "        [0.3404],\n",
      "        [0.3416],\n",
      "        [0.3416],\n",
      "        [0.3391],\n",
      "        [0.3419],\n",
      "        [0.3398],\n",
      "        [0.3416],\n",
      "        [0.3429],\n",
      "        [0.3381],\n",
      "        [0.3400],\n",
      "        [0.3418]], device='mps:0')\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.3414],\n",
      "        [0.3414],\n",
      "        [0.3372],\n",
      "        [0.3397],\n",
      "        [0.3408],\n",
      "        [0.3430],\n",
      "        [0.3383],\n",
      "        [0.3434],\n",
      "        [0.3412],\n",
      "        [0.3411],\n",
      "        [0.3397],\n",
      "        [0.3390],\n",
      "        [0.3389],\n",
      "        [0.3381],\n",
      "        [0.3453],\n",
      "        [0.3423],\n",
      "        [0.3404],\n",
      "        [0.3453],\n",
      "        [0.3402],\n",
      "        [0.3416],\n",
      "        [0.3360],\n",
      "        [0.3417],\n",
      "        [0.3391],\n",
      "        [0.3362],\n",
      "        [0.3444],\n",
      "        [0.3358],\n",
      "        [0.3398],\n",
      "        [0.3405],\n",
      "        [0.3440],\n",
      "        [0.3464],\n",
      "        [0.3399],\n",
      "        [0.3389]], device='mps:0')\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.3395],\n",
      "        [0.3410],\n",
      "        [0.3380],\n",
      "        [0.3415],\n",
      "        [0.3405],\n",
      "        [0.3390],\n",
      "        [0.3444],\n",
      "        [0.3380],\n",
      "        [0.3421],\n",
      "        [0.3378],\n",
      "        [0.3422],\n",
      "        [0.3448],\n",
      "        [0.3409],\n",
      "        [0.3400],\n",
      "        [0.3392],\n",
      "        [0.3437],\n",
      "        [0.3413],\n",
      "        [0.3391],\n",
      "        [0.3398],\n",
      "        [0.3415],\n",
      "        [0.3386],\n",
      "        [0.3420],\n",
      "        [0.3421],\n",
      "        [0.3381],\n",
      "        [0.3406],\n",
      "        [0.3390],\n",
      "        [0.3394],\n",
      "        [0.3410],\n",
      "        [0.3450],\n",
      "        [0.3397],\n",
      "        [0.3416],\n",
      "        [0.3427]], device='mps:0')\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([32, 128])\n",
      "DEBUG: x_out shape = torch.Size([32, 1])\n",
      "tensor([[0.3371],\n",
      "        [0.3424],\n",
      "        [0.3409],\n",
      "        [0.3397],\n",
      "        [0.3385],\n",
      "        [0.3404],\n",
      "        [0.3409],\n",
      "        [0.3425],\n",
      "        [0.3397],\n",
      "        [0.3388],\n",
      "        [0.3437],\n",
      "        [0.3433],\n",
      "        [0.3428],\n",
      "        [0.3387],\n",
      "        [0.3416],\n",
      "        [0.3402],\n",
      "        [0.3418],\n",
      "        [0.3412],\n",
      "        [0.3399],\n",
      "        [0.3422],\n",
      "        [0.3417],\n",
      "        [0.3387],\n",
      "        [0.3402],\n",
      "        [0.3426],\n",
      "        [0.3366],\n",
      "        [0.3386],\n",
      "        [0.3410],\n",
      "        [0.3404],\n",
      "        [0.3418],\n",
      "        [0.3398],\n",
      "        [0.3330],\n",
      "        [0.3420]], device='mps:0')\n",
      "DEBUG: x_pool shape before fc1 = torch.Size([13, 128])\n",
      "DEBUG: x_out shape = torch.Size([13, 1])\n",
      "tensor([[0.3474],\n",
      "        [0.3410],\n",
      "        [0.3406],\n",
      "        [0.3403],\n",
      "        [0.3377],\n",
      "        [0.3416],\n",
      "        [0.3445],\n",
      "        [0.3381],\n",
      "        [0.3390],\n",
      "        [0.3435],\n",
      "        [0.3427],\n",
      "        [0.3390],\n",
      "        [0.3409]], device='mps:0')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (141,1) (4435556,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m test_y = torch.cat([i.y \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m loader_test.dataset]).numpy()\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Accuracy\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m val_acc = (\u001b[43m(\u001b[49m\u001b[43mval_preds\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_y\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m).mean()\n\u001b[32m     21\u001b[39m test_acc = ((test_preds > \u001b[32m0.5\u001b[39m) == (test_y > \u001b[32m0.5\u001b[39m)).mean()\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mValidation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with shapes (141,1) (4435556,1) "
     ]
    }
   ],
   "source": [
    "# Run inference on model and extract val and test predictions\n",
    "def run_inference(model, loader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            preds.append(torch.sigmoid(model(batch).to(device)))\n",
    "        preds = torch.cat(preds)\n",
    "    return preds.cpu().numpy()\n",
    "\n",
    "test_preds = run_inference(model, loader_test)\n",
    "val_preds = run_inference(model, loader_val)\n",
    "\n",
    "# Truth values\n",
    "val_y = torch.cat([i.y for i in loader_val.dataset]).numpy()\n",
    "test_y = torch.cat([i.y for i in loader_test.dataset]).numpy()\n",
    "\n",
    "# Accuracy\n",
    "val_acc = ((val_preds > 0.5) == (val_y > 0.5)).mean()\n",
    "test_acc = ((test_preds > 0.5) == (test_y > 0.5)).mean()\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c065f991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.64206445],\n",
       "       [0.6422967 ],\n",
       "       [0.64196926],\n",
       "       [0.6422225 ],\n",
       "       [0.6422225 ],\n",
       "       [0.6420662 ],\n",
       "       [0.64199936],\n",
       "       [0.6422756 ],\n",
       "       [0.6420555 ],\n",
       "       [0.6420377 ],\n",
       "       [0.6421128 ],\n",
       "       [0.6422078 ],\n",
       "       [0.64195794],\n",
       "       [0.6420672 ],\n",
       "       [0.64235276],\n",
       "       [0.6420513 ],\n",
       "       [0.64204603],\n",
       "       [0.64221585],\n",
       "       [0.64186937],\n",
       "       [0.6421888 ],\n",
       "       [0.6420925 ],\n",
       "       [0.64205223],\n",
       "       [0.64212006],\n",
       "       [0.6421229 ],\n",
       "       [0.6419817 ],\n",
       "       [0.64213896],\n",
       "       [0.6420181 ],\n",
       "       [0.64212203],\n",
       "       [0.64219075],\n",
       "       [0.6419237 ],\n",
       "       [0.64202994],\n",
       "       [0.64213157],\n",
       "       [0.6421087 ],\n",
       "       [0.6421087 ],\n",
       "       [0.6418728 ],\n",
       "       [0.64201194],\n",
       "       [0.64207745],\n",
       "       [0.64219904],\n",
       "       [0.6419337 ],\n",
       "       [0.6422196 ],\n",
       "       [0.64210004],\n",
       "       [0.64209455],\n",
       "       [0.6420129 ],\n",
       "       [0.6419739 ],\n",
       "       [0.6419706 ],\n",
       "       [0.6419245 ],\n",
       "       [0.64232755],\n",
       "       [0.6421593 ],\n",
       "       [0.64205176],\n",
       "       [0.642329  ],\n",
       "       [0.6420411 ],\n",
       "       [0.6421193 ],\n",
       "       [0.64180833],\n",
       "       [0.64212847],\n",
       "       [0.6419827 ],\n",
       "       [0.6418213 ],\n",
       "       [0.6422779 ],\n",
       "       [0.64179546],\n",
       "       [0.6420193 ],\n",
       "       [0.64205897],\n",
       "       [0.6422529 ],\n",
       "       [0.64238644],\n",
       "       [0.6420278 ],\n",
       "       [0.64196813],\n",
       "       [0.64200294],\n",
       "       [0.64208686],\n",
       "       [0.6419199 ],\n",
       "       [0.6421175 ],\n",
       "       [0.6420597 ],\n",
       "       [0.6419765 ],\n",
       "       [0.64227504],\n",
       "       [0.64192027],\n",
       "       [0.6421509 ],\n",
       "       [0.6419093 ],\n",
       "       [0.6421536 ],\n",
       "       [0.64230084],\n",
       "       [0.6420837 ],\n",
       "       [0.64203095],\n",
       "       [0.6419886 ],\n",
       "       [0.6422382 ],\n",
       "       [0.64210385],\n",
       "       [0.641981  ],\n",
       "       [0.64202183],\n",
       "       [0.6421132 ],\n",
       "       [0.64195293],\n",
       "       [0.6421431 ],\n",
       "       [0.6421476 ],\n",
       "       [0.64192706],\n",
       "       [0.6420625 ],\n",
       "       [0.6419727 ],\n",
       "       [0.6419999 ],\n",
       "       [0.642085  ],\n",
       "       [0.6423078 ],\n",
       "       [0.64201444],\n",
       "       [0.64212215],\n",
       "       [0.6421839 ],\n",
       "       [0.64186746],\n",
       "       [0.6421633 ],\n",
       "       [0.6420807 ],\n",
       "       [0.64201415],\n",
       "       [0.6419469 ],\n",
       "       [0.64205146],\n",
       "       [0.6420814 ],\n",
       "       [0.6421683 ],\n",
       "       [0.64201474],\n",
       "       [0.6419642 ],\n",
       "       [0.64224017],\n",
       "       [0.6422157 ],\n",
       "       [0.6421868 ],\n",
       "       [0.64195716],\n",
       "       [0.6421178 ],\n",
       "       [0.6420446 ],\n",
       "       [0.64212924],\n",
       "       [0.64209574],\n",
       "       [0.64202595],\n",
       "       [0.6421551 ],\n",
       "       [0.6421277 ],\n",
       "       [0.64195967],\n",
       "       [0.6420408 ],\n",
       "       [0.6421763 ],\n",
       "       [0.6418434 ],\n",
       "       [0.6419513 ],\n",
       "       [0.6420873 ],\n",
       "       [0.64205176],\n",
       "       [0.64213014],\n",
       "       [0.6420208 ],\n",
       "       [0.64164174],\n",
       "       [0.6421409 ],\n",
       "       [0.6424415 ],\n",
       "       [0.6420878 ],\n",
       "       [0.64206696],\n",
       "       [0.6420481 ],\n",
       "       [0.6419031 ],\n",
       "       [0.6421184 ],\n",
       "       [0.64228135],\n",
       "       [0.64192295],\n",
       "       [0.6419765 ],\n",
       "       [0.642225  ],\n",
       "       [0.642184  ],\n",
       "       [0.6419743 ],\n",
       "       [0.6420815 ]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "497d0f7a-e5a7-4cff-9054-e32ae89dd143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/00.02\n",
      "shape preds_test: (177, 1)\n",
      "shape test_y: (5472684, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (177,1) (5472684,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mshape preds_test: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreds_test.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mshape test_y: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_y.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     results[p]={\u001b[33m'\u001b[39m\u001b[33mtest_accuracy\u001b[39m\u001b[33m'\u001b[39m: (\u001b[43mpreds_test\u001b[49m\u001b[43m==\u001b[49m\u001b[43mtest_y\u001b[49m).astype(\u001b[38;5;28mfloat\u001b[39m).mean(),\n\u001b[32m      9\u001b[39m      \u001b[33m'\u001b[39m\u001b[33mval_accuracy\u001b[39m\u001b[33m'\u001b[39m:(preds_val==val_y).astype(\u001b[38;5;28mfloat\u001b[39m).mean()}\n\u001b[32m     10\u001b[39m     results = pd.DataFrame(results)\n\u001b[32m     11\u001b[39m results.columns = [os.path.split(i)[-\u001b[32m1\u001b[39m].replace(\u001b[33m'\u001b[39m\u001b[33m.pkl\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m results.columns]\n",
      "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with shapes (177,1) (5472684,1) "
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "print(PATH_DATA)\n",
    "for p in glob(os.path.join(PATH_DATA,'*.pkl')): \n",
    "    preds_test = joblib.load(p)['preds_test']>=0.5\n",
    "    preds_val =joblib.load(p)['preds_val']>=0.5\n",
    "    print(f\"shape preds_test: {preds_test.shape}\")\n",
    "    print(f\"shape test_y: {test_y.shape}\")\n",
    "    results[p]={'test_accuracy': (preds_test==test_y).astype(float).mean(),\n",
    "     'val_accuracy':(preds_val==val_y).astype(float).mean()}\n",
    "    results = pd.DataFrame(results)\n",
    "results.columns = [os.path.split(i)[-1].replace('.pkl','') for i in results.columns]\n",
    "results = results.T.sort_values('val_accuracy', ascending = False)\n",
    "results.index = pd.MultiIndex.from_tuples([tuple(i.split('_')) for i in results.index])\n",
    "results.index.names = ['#layers','#params']\n",
    "results.to_csv(os.path.join(PATH_DATA, 'results.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3305321c-ab2e-4b06-a70d-f1ce2e6580ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#layers</th>\n",
       "      <th>#params</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>1000000</th>\n",
       "      <td>0.994750</td>\n",
       "      <td>0.994493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500000</th>\n",
       "      <td>0.994401</td>\n",
       "      <td>0.994209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>1000000</th>\n",
       "      <td>0.993834</td>\n",
       "      <td>0.993719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>100000</th>\n",
       "      <td>0.993913</td>\n",
       "      <td>0.993667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>500000</th>\n",
       "      <td>0.993656</td>\n",
       "      <td>0.993433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>1000000</th>\n",
       "      <td>0.992984</td>\n",
       "      <td>0.992853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500000</th>\n",
       "      <td>0.992865</td>\n",
       "      <td>0.992497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>100000</th>\n",
       "      <td>0.992658</td>\n",
       "      <td>0.992375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>100000</th>\n",
       "      <td>0.990710</td>\n",
       "      <td>0.990459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 test_accuracy  val_accuracy\n",
       "#layers #params                             \n",
       "4       1000000       0.994750      0.994493\n",
       "        500000        0.994401      0.994209\n",
       "3       1000000       0.993834      0.993719\n",
       "4       100000        0.993913      0.993667\n",
       "3       500000        0.993656      0.993433\n",
       "2       1000000       0.992984      0.992853\n",
       "        500000        0.992865      0.992497\n",
       "3       100000        0.992658      0.992375\n",
       "2       100000        0.990710      0.990459"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0e3dd0ba-e981-4c47-852d-14c66aeb5c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy 0.9947501079908871\n"
     ]
    }
   ],
   "source": [
    "print('The Accuracy', results.iloc[0]['test_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d4d99c-701d-4b02-9e24-cf80034fd80a",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e8473a-996c-46a5-8dec-88948ff9da82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd971d8-879e-4a50-bbcb-84323ccb0220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359bd6be-ce59-40f1-98d9-d61e6b6017d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "543661b5-fc2d-4d4d-b2eb-d05270fbbd15",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asdfasdfasfd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m asdfasdfasfd\n",
      "\u001b[31mNameError\u001b[39m: name 'asdfasdfasfd' is not defined"
     ]
    }
   ],
   "source": [
    "asdfasdfasfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c828c0e9-5b48-4bc7-b401-4f620fd682f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for n_layers in tqdm([3,2,4]): \n",
    "    for target_params in tqdm([100_000,500_000,1_000_000]): \n",
    "        print(n_layers, target_params)\n",
    "        # Find out the hyperparameteres yielding #params = target_params\n",
    "        def objective(h):\n",
    "            return count_parameters(InteractionNetwork(int(h),n_layers)) - target_params\n",
    "        optimal_h = int(root_scalar(objective, bracket=[1, 3000], method='bisect').root)\n",
    "        optimal_h= pd.Series({optimal_h:target_params-count_parameters(InteractionNetwork(optimal_h, n_layers)),\n",
    "                    optimal_h-1:target_params-count_parameters(InteractionNetwork(optimal_h-1,n_layers)),\n",
    "                    optimal_h+1:target_params-count_parameters(InteractionNetwork(optimal_h+1,n_layers))}).abs().idxmin()\n",
    "        \n",
    "        model = InteractionNetwork(optimal_h,n_layers).to(device)\n",
    "        lr = LR\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "        best_val_loss = float('inf')\n",
    "        epochs_no_improve, epochs_no_improve2 = 0,0\n",
    "        best_model_state = None\n",
    "        stats = []\n",
    "        best = None\n",
    "        # Print header once\n",
    "        print(f\"{'Epoch':>5} | {'Train Loss':>10} | {'Val Loss':>9} | {'Val Acc':>8} | {'Test Acc':>9}\")\n",
    "        print(\"-\" * 50)\n",
    "        for epoch in trange(MAX_EPOCHS):\n",
    "            train_loss = train_epoch(model, loader_train, optimizer)   \n",
    "            preds_val, actuals_val, acc_val, val_loss = evaluate(model,loader_val)\n",
    "            preds_test, actuals_test, acc_test, test_loss = evaluate(model,loader_test)\n",
    "            \n",
    "            stats.append({'train_loss':train_loss, 'val_loss':val_loss, 'acc_val':acc_val, 'acc_test':acc_test})\n",
    "            if val_loss < best_val_loss: \n",
    "                print(f\"{epoch+1:5d} | {train_loss:10.4f} | {val_loss:9.4f} | {acc_val:8.4f} | {acc_test:9.4f} *\")\n",
    "                best_val_loss = val_loss\n",
    "                epochs_no_improve = 0\n",
    "                epochs_no_improve2 = 0\n",
    "                best = {'model_state': {k: v.cpu() for k, v in model.state_dict().items()},\n",
    "                        'preds_test':preds_test, 'preds_val':preds_val}        \n",
    "            else:\n",
    "                print(f\"{epoch+1:5d} | {train_loss:10.4f} | {val_loss:9.4f} | {acc_val:8.4f} | {acc_test:9.4f}\")\n",
    "                epochs_no_improve += 1\n",
    "                epochs_no_improve2 += 1\n",
    "        \n",
    "            if epochs_no_improve >= TOLERANCE:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "            if epochs_no_improve2 >= LR_TOLERANCE:\n",
    "                if lr >=1.0e-8:\n",
    "                    lr/=10\n",
    "                print(f\"LR reduction to {lr}\")\n",
    "        os.makedirs(PATH_DATA, exist_ok=True)\n",
    "        joblib.dump(best, os.path.join(PATH_DATA, f\"{n_layers}_{target_params}.pkl\"))\n",
    "        \n",
    "        stats = pd.DataFrame(stats)\n",
    "        stats[['train_loss','val_loss']].plot(figsize = (15,4))\n",
    "        plt.show()\n",
    "        stats[['acc_val','acc_test']].plot(figsize = (15,4))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5612fe45-35a1-4990-8c4f-0fb27f96a6ad",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409092eb-d3db-4317-8f16-532759f3a52b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn-class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
