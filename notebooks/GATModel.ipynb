{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03ce9727-c8eb-490e-ba89-e6e5100c4a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import torch_geometric\n",
    "from tqdm import tqdm, trange\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "import gc\n",
    "import joblib\n",
    "from joblib import delayed, Parallel\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ba2fada-5b43-46ae-87e9-00fbd574bf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c3a606d-b257-42b5-a4ea-5cf1657709dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA0 = './data/00.01'\n",
    "PATH_DATA = './data/00.02'\n",
    "RANDOM_SEED =0\n",
    "np.random.seed(RANDOM_SEED)  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e384185-8be8-48ab-a208-0b7bea98be20",
   "metadata": {},
   "outputs": [],
   "source": [
    "CRITERION = nn.BCEWithLogitsLoss()\n",
    "LR = 0.001\n",
    "TOLERANCE = 20\n",
    "LR_TOLERANCE= 5\n",
    "MAX_EPOCHS = 200\n",
    "BATCH_SIZE =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a2d1d06-1abd-4b8f-8525-172150e7a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = torch_geometric.loader.DataLoader(\n",
    "    pd.read_pickle(os.path.join('max_prob_10_subsample_0.1','graphs_train.pkl')).tolist(),\n",
    "    batch_size = BATCH_SIZE,shuffle = True)\n",
    "loader_val = torch_geometric.loader.DataLoader(\n",
    "    pd.read_pickle(os.path.join('max_prob_10_subsample_0.1','graphs_val.pkl')).tolist(),batch_size = BATCH_SIZE\n",
    "    ,shuffle = False)\n",
    "loader_test = torch_geometric.loader.DataLoader(\n",
    "    pd.read_pickle(os.path.join('max_prob_10_subsample_0.1','graphs_test.pkl')).tolist(),batch_size = BATCH_SIZE\n",
    "    ,shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23de33a3-b526-4ea2-bf0b-a8c36ac8e7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3207e-01, -1.9634e-02,  2.4826e-01, -6.9013e-01],\n",
       "        [-4.6413e-01, -4.6385e-02,  4.9653e-01, -6.6614e-01],\n",
       "        [-6.2136e-01, -6.2315e-02,  6.6473e-01, -6.5209e-01],\n",
       "        ...,\n",
       "        [-4.2989e-01,  5.0476e+00, -4.9280e-01,  4.4754e+00],\n",
       "        [-4.9400e-01, -1.7182e-01, -5.0646e-01, -4.8015e-01],\n",
       "        [ 6.6460e-03, -1.7567e-03,  3.7240e-03, -5.6318e-01]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader_train.dataset[0].edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec75db79-80ac-4c07-9345-7dd800cc5eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, activation, heads,\n",
    "                 node_emb_dim, edge_attr_dim, dropout = 0): #add edge_attr_dim\n",
    "        #first get node embeddings via attention ADD EDGES TO GATCONV\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.acts = nn.ModuleList()\n",
    "\n",
    "        self.convs.append(\n",
    "            GATConv(in_channels, hidden_channels, heads=heads, edge_dim = edge_attr_dim, dropout=dropout)\n",
    "        )\n",
    "        self.acts.append(activation)\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(\n",
    "                GATConv(hidden_channels * heads,\n",
    "                        hidden_channels,\n",
    "                        heads=heads,\n",
    "                        dropout=dropout,\n",
    "                        edge_dim = edge_attr_dim,\n",
    "                        concat = True)\n",
    "            )\n",
    "            self.acts.append(activation)\n",
    "\n",
    "        self.convs.append(\n",
    "            GATConv(hidden_channels * heads,\n",
    "                    node_emb_dim,\n",
    "                    heads=heads,\n",
    "                    edge_dim = edge_attr_dim,\n",
    "                    concat=False,\n",
    "                    dropout=dropout)\n",
    "        )\n",
    "\n",
    "        #second, get edge classification via MLP\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(2 * node_emb_dim + edge_attr_dim, hidden_channels),\n",
    "            activation,\n",
    "            #nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        for conv, act in zip(self.convs[:-1], self.acts):\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            x = act(x)\n",
    "        node_emb = self.convs[-1](x, edge_index, edge_attr)\n",
    "\n",
    "\n",
    "        src, dst = edge_index\n",
    "        src, dst = torch.minimum(src, dst), torch.maximum(src, dst) #make sure one edge embedding is invariant\n",
    "        h_src = node_emb[src]\n",
    "        h_dst = node_emb[dst]\n",
    "\n",
    "        edge_feat = torch.cat([h_src, h_dst, edge_attr], dim = 1)\n",
    "        logits = self.edge_mlp(edge_feat).view(-1)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df32845b-f0c1-46dd-ae8a-9883311a8624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3615377f-6dd8-4aec-936e-ac09e8a8f0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, loss_fn, device=None):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "\n",
    "    for batch in loader:\n",
    "        if device is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "\n",
    "        loss = loss_fn(out, batch.y.view(-1).float()) #batch.y.view(-1).float()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "231e90c2-6af0-4f5d-a7a4-7f5bf7c60667",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, loader, loss_fn, device=None):\n",
    "  model.eval()\n",
    "  preds, actuals = [], []\n",
    "\n",
    "  for batch in loader:\n",
    "    if device is not None:\n",
    "      batch = batch.to(device)\n",
    "\n",
    "    logits = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds.append(probs.cpu())\n",
    "    actuals.append(batch.y.cpu())\n",
    "\n",
    "  preds = torch.cat(preds).flatten()\n",
    "  actuals = torch.cat(actuals).flatten()\n",
    "  acc = ((preds > 0.5) == (actuals > 0.5)).type(torch.float).mean().item()\n",
    "  loss = loss_fn(preds, actuals.float()).item()\n",
    "\n",
    "  return preds.numpy(), actuals.numpy(), acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c4fd504-01dd-4b03-8188-1c3c77636439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader_train.dataset[0].edge_attr.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10041adf-7fa5-47f9-b2d7-aaab65b3e6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_gat_hyperparams(in_channels, out_channels, n_layers, activation, edge_attr_dim, target_params, head_choices = (1,2,4,8),\n",
    "                              hidden_choices = (16,32,64,128,256,512), emb_choices = (16,32,64,128,256,512)):\n",
    "    best = None\n",
    "    for heads in head_choices:\n",
    "      for hidden in hidden_choices:\n",
    "        for emb in emb_choices:\n",
    "          model = GraphAttentionModel(\n",
    "              in_channels = in_channels,\n",
    "              hidden_channels = hidden,\n",
    "              out_channels = out_channels,\n",
    "              num_layers = n_layers,\n",
    "              activation = activation,\n",
    "              heads = heads,\n",
    "              node_emb_dim = emb,\n",
    "              edge_attr_dim = edge_attr_dim,\n",
    "          )\n",
    "          p = count_parameters(model)\n",
    "          diff = abs(p-target_params)\n",
    "          if best is None or diff < best[0]:\n",
    "            best = (diff, heads, hidden, emb, p)\n",
    "    return best[1], best[2], best[3], best[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7498799b-4217-4b2f-a070-9b240a6d0f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGE_ATTR_DIM = loader_train.dataset[0].edge_attr.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33486c6f-33f9-4bb0-adb6-c7990d407ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paper for GAT uses 4 heads for inductive task, so we do that too.\n",
    "layers_to_hyperparams_2 = {} #key = # (layers, target_params), value = (#heads, hidden dim, embedding dim, # params) output of find_best_gat_hyperparams\n",
    "for i,j in itertools.product([2,3,4], [100000, 500000, 1000000]):\n",
    "  layers_to_hyperparams_2[(i,j)] = find_best_gat_hyperparams(in_channels= loader_train.dataset[0].num_node_features,\n",
    "                                                      out_channels=loader_train.dataset[0].y.size(-1),  # or number of classes\n",
    "                                                      n_layers=i,\n",
    "                                                      activation=nn.ReLU(),\n",
    "                                                      edge_attr_dim = EDGE_ATTR_DIM,\n",
    "                                                      target_params=j,\n",
    "                                                      head_choices = (4,),\n",
    "                                                      hidden_choices = (64,128,256), \n",
    "                                                      emb_choices = (64,128,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d188b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_to_hyperparams_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22031a80-8d30-4980-b314-8e991e744f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gat_for(n_layers, target_params, lth = layers_to_hyperparams_2, path_data = PATH_DATA, **model_kwargs):\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "    print(f\"\\n>> Layers={n_layers}, target≈{target_params:,} params\")\n",
    "\n",
    "    # ---- grab the precomputed tuple, instead of re‐searching ----\n",
    "    try:\n",
    "        heads, hidden, emb, actual = lth[(n_layers, target_params)]\n",
    "    except KeyError:\n",
    "        raise KeyError(f\"No entry in lth for {(n_layers, target_params)}\")\n",
    "\n",
    "    print(f\"→ Using cached heads={heads}, hidden={hidden}, emb={emb} → {actual:,} params\")\n",
    "\n",
    "\n",
    "    in_dim  = loader_train.dataset[0].num_node_features\n",
    "    out_dim = loader_train.dataset[0].y.size(-1)\n",
    "    edge_attr_dim = 4\n",
    "    ACT, LR = nn.ReLU(), 1e-3\n",
    "    lr = LR\n",
    "\n",
    "    model = GraphAttentionModel(\n",
    "        in_channels    = in_dim,\n",
    "        hidden_channels= hidden,\n",
    "        out_channels   = out_dim,\n",
    "        num_layers     = n_layers,\n",
    "        activation     = ACT,\n",
    "        heads          = heads,\n",
    "        node_emb_dim   = emb,\n",
    "        edge_attr_dim  = edge_attr_dim,\n",
    "        **model_kwargs\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    best_val, epochs_no_imp, epochs_no_imp2 = float('inf'), 0, 0\n",
    "    stats, best = [], {}\n",
    "\n",
    "    print(f\"{'E':>3} | {'Train L':>7} | {'Val L':>6} | {'Val Acc':>7} | {'Test Acc':>8}\")\n",
    "    print(\"-\"*45)\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        tr_loss = train(model, loader_train, optimizer, CRITERION, device)\n",
    "        pv, av, accv, lv = test (model, loader_val,   CRITERION, device)\n",
    "        pt, at, acct, lt = test (model, loader_test,  CRITERION, device)\n",
    "\n",
    "        stats.append({'train_loss':tr_loss,'val_loss':lv,'acc_val':accv,'acc_test':acct})\n",
    "        star = \"\"\n",
    "        if lv < best_val:\n",
    "            best_val = lv\n",
    "            epochs_no_imp = epochs_no_imp2 = 0\n",
    "            best = {\n",
    "                'state':     {k:v.cpu() for k,v in model.state_dict().items()},\n",
    "                'preds_val': pv, 'preds_test': pt\n",
    "            }\n",
    "            star = \"*\"\n",
    "        else:\n",
    "            epochs_no_imp  += 1\n",
    "            epochs_no_imp2 += 1\n",
    "\n",
    "        print(f\"{epoch+1:3d} | {tr_loss:7.4f} | {lv:6.4f} | {accv:7.4f} | {acct:8.4f} {star}\")\n",
    "\n",
    "        if epochs_no_imp  >= TOLERANCE:\n",
    "            print(f\"→ Early stopping @ epoch {epoch+1}\")\n",
    "            break\n",
    "        if epochs_no_imp2 >= LR_TOLERANCE:\n",
    "            if lr >=1.0e-8:\n",
    "                lr /= 10\n",
    "                for g in optimizer.param_groups:\n",
    "                    g['lr'] = lr\n",
    "            print(f\"→ LR reduced to {lr:e}\")\n",
    "\n",
    "    best['stats'] = pd.DataFrame(stats)\n",
    "    os.makedirs(path_data, exist_ok=True)\n",
    "    joblib.dump(best, os.path.join(path_data, f\"gat_{n_layers}_{target_params}.pkl\"))\n",
    "\n",
    "    # optional plotting\n",
    "    #best['stats'][['train_loss','val_loss']].plot(figsize=(12,3)); plt.show()\n",
    "    #best['stats'][['acc_val','acc_test']].plot(figsize=(12,3)); plt.show()\n",
    "\n",
    "    del model, optimizer, stats\n",
    "    gc.collect(); torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e20f3b67-7514-4cd7-996a-1478998137c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA3 = './data/00.03'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ed5fd3a-a4da-46c9-af75-aac5b4c62b0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[A%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Layers=4, target≈500,000 params\n",
      "→ Using cached heads=4, hidden=64, emb=256 → 440,705 params\n",
      "  E | Train L |  Val L | Val Acc | Test Acc\n",
      "---------------------------------------------\n",
      "  1 |  0.6205 | 0.4348 |  0.9042 |   0.9067 *\n",
      "  2 |  0.4006 | 0.4217 |  0.9205 |   0.9233 *\n",
      "  3 |  0.3527 | 0.4167 |  0.9265 |   0.9291 *\n",
      "  4 |  0.3268 | 0.4139 |  0.9315 |   0.9339 *\n",
      "  5 |  0.3026 | 0.4112 |  0.9378 |   0.9399 *\n",
      "  6 |  0.2799 | 0.4079 |  0.9437 |   0.9456 *\n",
      "  7 |  0.2567 | 0.4057 |  0.9481 |   0.9499 *\n",
      "  8 |  0.2418 | 0.4026 |  0.9501 |   0.9518 *\n",
      "  9 |  0.2293 | 0.4020 |  0.9524 |   0.9538 *\n",
      " 10 |  0.2183 | 0.4009 |  0.9542 |   0.9551 *\n",
      " 11 |  0.2079 | 0.3991 |  0.9570 |   0.9582 *\n",
      " 12 |  0.1999 | 0.3985 |  0.9577 |   0.9584 *\n",
      " 13 |  0.1928 | 0.3975 |  0.9584 |   0.9598 *\n",
      " 14 |  0.1870 | 0.3969 |  0.9614 |   0.9625 *\n",
      " 15 |  0.1808 | 0.3959 |  0.9621 |   0.9631 *\n",
      " 16 |  0.1744 | 0.3955 |  0.9615 |   0.9627 *\n",
      " 17 |  0.1699 | 0.3951 |  0.9637 |   0.9644 *\n",
      " 18 |  0.1662 | 0.3945 |  0.9642 |   0.9651 *\n",
      " 19 |  0.1605 | 0.3935 |  0.9647 |   0.9652 *\n",
      " 20 |  0.1556 | 0.3940 |  0.9640 |   0.9653 \n",
      " 21 |  0.1526 | 0.3926 |  0.9658 |   0.9666 *\n",
      " 22 |  0.1488 | 0.3925 |  0.9666 |   0.9671 *\n",
      " 23 |  0.1460 | 0.3919 |  0.9666 |   0.9672 *\n",
      " 24 |  0.1419 | 0.3922 |  0.9672 |   0.9681 \n",
      " 25 |  0.1391 | 0.3914 |  0.9676 |   0.9681 *\n",
      " 26 |  0.1359 | 0.3914 |  0.9672 |   0.9679 \n",
      " 27 |  0.1342 | 0.3917 |  0.9663 |   0.9669 \n",
      " 28 |  0.1320 | 0.3907 |  0.9685 |   0.9692 *\n",
      " 29 |  0.1280 | 0.3908 |  0.9690 |   0.9697 \n",
      " 30 |  0.1254 | 0.3899 |  0.9692 |   0.9696 *\n",
      " 31 |  0.1236 | 0.3901 |  0.9694 |   0.9701 \n",
      " 32 |  0.1216 | 0.3900 |  0.9691 |   0.9699 \n",
      " 33 |  0.1185 | 0.3905 |  0.9684 |   0.9691 \n",
      " 34 |  0.1167 | 0.3897 |  0.9697 |   0.9704 *\n",
      " 35 |  0.1150 | 0.3895 |  0.9699 |   0.9709 *\n",
      " 36 |  0.1137 | 0.3900 |  0.9688 |   0.9700 \n",
      " 37 |  0.1114 | 0.3891 |  0.9701 |   0.9708 *\n",
      " 38 |  0.1097 | 0.3894 |  0.9702 |   0.9711 \n",
      " 39 |  0.1077 | 0.3887 |  0.9704 |   0.9715 *\n",
      " 40 |  0.1057 | 0.3887 |  0.9702 |   0.9708 *\n",
      " 41 |  0.1051 | 0.3883 |  0.9711 |   0.9717 *\n",
      " 42 |  0.1036 | 0.3885 |  0.9706 |   0.9713 \n",
      " 43 |  0.1007 | 0.3881 |  0.9710 |   0.9717 *\n",
      " 44 |  0.0995 | 0.3885 |  0.9709 |   0.9716 \n",
      " 45 |  0.0995 | 0.3883 |  0.9714 |   0.9721 \n",
      " 46 |  0.0970 | 0.3884 |  0.9711 |   0.9722 \n",
      " 47 |  0.0965 | 0.3882 |  0.9708 |   0.9716 \n",
      " 48 |  0.0957 | 0.3880 |  0.9711 |   0.9719 *\n",
      " 49 |  0.0944 | 0.3877 |  0.9714 |   0.9723 *\n",
      " 50 |  0.0952 | 0.3880 |  0.9709 |   0.9715 \n",
      " 51 |  0.0914 | 0.3874 |  0.9716 |   0.9724 *\n",
      " 52 |  0.0891 | 0.3878 |  0.9717 |   0.9726 \n",
      " 53 |  0.0888 | 0.3870 |  0.9712 |   0.9721 *\n",
      " 54 |  0.0883 | 0.3874 |  0.9715 |   0.9723 \n",
      " 55 |  0.0880 | 0.3871 |  0.9718 |   0.9726 \n",
      " 56 |  0.0875 | 0.3869 |  0.9721 |   0.9727 *\n",
      " 57 |  0.0849 | 0.3873 |  0.9722 |   0.9730 \n",
      " 58 |  0.0835 | 0.3872 |  0.9722 |   0.9730 \n",
      " 59 |  0.0835 | 0.3870 |  0.9720 |   0.9732 \n",
      " 60 |  0.0824 | 0.3871 |  0.9721 |   0.9730 \n",
      " 61 |  0.0817 | 0.3868 |  0.9725 |   0.9733 *\n",
      " 62 |  0.0812 | 0.3870 |  0.9722 |   0.9729 \n",
      " 63 |  0.0798 | 0.3870 |  0.9713 |   0.9719 \n",
      " 64 |  0.0795 | 0.3868 |  0.9722 |   0.9731 *\n",
      " 65 |  0.0790 | 0.3867 |  0.9723 |   0.9732 *\n",
      " 66 |  0.0782 | 0.3869 |  0.9723 |   0.9733 \n",
      " 67 |  0.0776 | 0.3865 |  0.9723 |   0.9732 *\n",
      " 68 |  0.0745 | 0.3864 |  0.9721 |   0.9727 *\n",
      " 69 |  0.0754 | 0.3865 |  0.9726 |   0.9736 \n",
      " 70 |  0.0752 | 0.3863 |  0.9726 |   0.9734 *\n",
      " 71 |  0.0746 | 0.3865 |  0.9727 |   0.9734 \n",
      " 72 |  0.0739 | 0.3865 |  0.9726 |   0.9734 \n",
      " 73 |  0.0731 | 0.3861 |  0.9727 |   0.9735 *\n",
      " 74 |  0.0714 | 0.3865 |  0.9716 |   0.9722 \n",
      " 75 |  0.0736 | 0.3876 |  0.9711 |   0.9719 \n",
      " 76 |  0.0731 | 0.3861 |  0.9728 |   0.9735 \n",
      " 77 |  0.0703 | 0.3863 |  0.9727 |   0.9737 \n",
      " 78 |  0.0693 | 0.3862 |  0.9729 |   0.9738 \n",
      "→ LR reduced to 1.000000e-04\n",
      " 79 |  0.0599 | 0.3856 |  0.9736 |   0.9745 *\n",
      " 80 |  0.0571 | 0.3856 |  0.9737 |   0.9745 \n",
      " 81 |  0.0563 | 0.3857 |  0.9736 |   0.9745 \n",
      " 82 |  0.0559 | 0.3857 |  0.9736 |   0.9744 \n",
      " 83 |  0.0552 | 0.3856 |  0.9736 |   0.9744 *\n",
      " 84 |  0.0550 | 0.3857 |  0.9734 |   0.9743 \n",
      " 85 |  0.0545 | 0.3856 |  0.9735 |   0.9743 \n",
      " 86 |  0.0543 | 0.3857 |  0.9733 |   0.9742 \n",
      " 87 |  0.0541 | 0.3857 |  0.9733 |   0.9742 \n",
      " 88 |  0.0539 | 0.3858 |  0.9733 |   0.9741 \n",
      "→ LR reduced to 1.000000e-05\n",
      " 89 |  0.0529 | 0.3856 |  0.9733 |   0.9742 \n",
      "→ LR reduced to 1.000000e-06\n",
      " 90 |  0.0525 | 0.3857 |  0.9733 |   0.9742 \n",
      "→ LR reduced to 1.000000e-07\n",
      " 91 |  0.0524 | 0.3857 |  0.9733 |   0.9742 \n",
      "→ LR reduced to 1.000000e-08\n",
      " 92 |  0.0527 | 0.3857 |  0.9733 |   0.9742 \n",
      "→ LR reduced to 1.000000e-09\n",
      " 93 |  0.0523 | 0.3857 |  0.9733 |   0.9742 \n",
      "→ LR reduced to 1.000000e-09\n",
      " 94 |  0.0525 | 0.3857 |  0.9733 |   0.9742 \n",
      "→ LR reduced to 1.000000e-09\n",
      " 95 |  0.0525 | 0.3857 |  0.9733 |   0.9742 \n",
      "→ LR reduced to 1.000000e-09\n",
      " 96 |  0.0525 | 0.3857 |  0.9733 |   0.9742 \n",
      "→ LR reduced to 1.000000e-09\n",
      " 97 |  0.0525 | 0.3857 |  0.9733 |   0.9742 \n",
      "→ LR reduced to 1.000000e-09\n",
      " 98 |  0.0526 | 0.3857 |  0.9733 |   0.9742 \n",
      "→ LR reduced to 1.000000e-09\n",
      " 99 |  0.0525 | 0.3857 |  0.9733 |   0.9742 \n",
      "→ LR reduced to 1.000000e-09\n",
      "100 |  0.0524 | 0.3857 |  0.9733 |   0.9742 \n",
      "→ LR reduced to 1.000000e-09\n",
      "101 |  0.0525 | 0.3857 |  0.9733 |   0.9742 \n",
      "→ LR reduced to 1.000000e-09\n",
      "102 |  0.0526 | 0.3857 |  0.9733 |   0.9742 \n",
      "→ LR reduced to 1.000000e-09\n",
      "103 |  0.0526 | 0.3857 |  0.9733 |   0.9742 \n",
      "→ Early stopping @ epoch 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A%|█████████████████████████████████████████                                         | 1/2 [17:10<17:10, 1030.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Layers=4, target≈1,000,000 params\n",
      "→ Using cached heads=4, hidden=128, emb=256 → 1,136,129 params\n",
      "  E | Train L |  Val L | Val Acc | Test Acc\n",
      "---------------------------------------------\n",
      "  1 |  0.6059 | 0.4307 |  0.9049 |   0.9079 *\n",
      "  2 |  0.3747 | 0.4169 |  0.9271 |   0.9292 *\n",
      "  3 |  0.3170 | 0.4105 |  0.9386 |   0.9407 *\n",
      "  4 |  0.2700 | 0.4051 |  0.9467 |   0.9484 *\n",
      "  5 |  0.2411 | 0.4038 |  0.9521 |   0.9536 *\n",
      "  6 |  0.2228 | 0.4000 |  0.9539 |   0.9557 *\n",
      "  7 |  0.2066 | 0.3982 |  0.9591 |   0.9601 *\n",
      "  8 |  0.1912 | 0.3974 |  0.9593 |   0.9607 *\n",
      "  9 |  0.1785 | 0.3955 |  0.9637 |   0.9650 *\n",
      " 10 |  0.1679 | 0.3937 |  0.9650 |   0.9664 *\n",
      " 11 |  0.1568 | 0.3927 |  0.9688 |   0.9695 *\n",
      " 12 |  0.1489 | 0.3912 |  0.9695 |   0.9704 *\n",
      " 13 |  0.1404 | 0.3908 |  0.9687 |   0.9699 *\n",
      " 14 |  0.1340 | 0.3899 |  0.9721 |   0.9728 *\n",
      " 15 |  0.1277 | 0.3889 |  0.9727 |   0.9733 *\n",
      " 16 |  0.1210 | 0.3882 |  0.9744 |   0.9750 *\n",
      " 17 |  0.1151 | 0.3872 |  0.9744 |   0.9749 *\n",
      " 18 |  0.1115 | 0.3870 |  0.9756 |   0.9760 *\n",
      " 19 |  0.1058 | 0.3861 |  0.9757 |   0.9761 *\n",
      " 20 |  0.1028 | 0.3857 |  0.9767 |   0.9772 *\n",
      " 21 |  0.0977 | 0.3861 |  0.9759 |   0.9768 \n",
      " 22 |  0.0946 | 0.3854 |  0.9774 |   0.9778 *\n",
      " 23 |  0.0918 | 0.3853 |  0.9768 |   0.9775 *\n",
      " 24 |  0.0898 | 0.3849 |  0.9781 |   0.9784 *\n",
      " 25 |  0.0858 | 0.3844 |  0.9784 |   0.9790 *\n",
      " 26 |  0.0844 | 0.3844 |  0.9787 |   0.9791 \n",
      " 27 |  0.0804 | 0.3847 |  0.9773 |   0.9783 \n",
      " 28 |  0.0775 | 0.3839 |  0.9788 |   0.9792 *\n",
      " 29 |  0.0761 | 0.3837 |  0.9789 |   0.9790 *\n",
      " 30 |  0.0743 | 0.3839 |  0.9786 |   0.9791 \n",
      " 31 |  0.0724 | 0.3835 |  0.9790 |   0.9792 *\n",
      " 32 |  0.0696 | 0.3832 |  0.9795 |   0.9798 *\n",
      " 33 |  0.0682 | 0.3831 |  0.9795 |   0.9796 *\n",
      " 34 |  0.0660 | 0.3832 |  0.9794 |   0.9797 \n",
      " 35 |  0.0660 | 0.3833 |  0.9790 |   0.9794 \n",
      " 36 |  0.0639 | 0.3831 |  0.9795 |   0.9798 *\n",
      " 37 |  0.0606 | 0.3826 |  0.9801 |   0.9803 *\n",
      " 38 |  0.0605 | 0.3825 |  0.9794 |   0.9797 *\n",
      " 39 |  0.0611 | 0.3827 |  0.9796 |   0.9797 \n",
      " 40 |  0.0567 | 0.3825 |  0.9798 |   0.9800 *\n",
      " 41 |  0.0586 | 0.3825 |  0.9797 |   0.9796 *\n",
      " 42 |  0.0558 | 0.3821 |  0.9801 |   0.9802 *\n",
      " 43 |  0.0527 | 0.3821 |  0.9800 |   0.9802 *\n",
      " 44 |  0.0537 | 0.3824 |  0.9801 |   0.9802 \n",
      " 45 |  0.0524 | 0.3820 |  0.9805 |   0.9809 *\n",
      " 46 |  0.0526 | 0.3822 |  0.9803 |   0.9804 \n",
      " 47 |  0.0502 | 0.3821 |  0.9807 |   0.9808 \n",
      " 48 |  0.0490 | 0.3818 |  0.9802 |   0.9803 *\n",
      " 49 |  0.0474 | 0.3818 |  0.9804 |   0.9805 *\n",
      " 50 |  0.0474 | 0.3824 |  0.9801 |   0.9803 \n",
      " 51 |  0.0467 | 0.3816 |  0.9809 |   0.9808 *\n",
      " 52 |  0.0466 | 0.3823 |  0.9799 |   0.9802 \n",
      " 53 |  0.0462 | 0.3817 |  0.9805 |   0.9802 \n",
      " 54 |  0.0447 | 0.3818 |  0.9800 |   0.9802 \n",
      " 55 |  0.0445 | 0.3817 |  0.9806 |   0.9807 \n",
      " 56 |  0.0424 | 0.3814 |  0.9804 |   0.9804 *\n",
      " 57 |  0.0430 | 0.3817 |  0.9799 |   0.9801 \n",
      " 58 |  0.0428 | 0.3817 |  0.9801 |   0.9803 \n",
      " 59 |  0.0420 | 0.3818 |  0.9802 |   0.9804 \n",
      " 60 |  0.0412 | 0.3819 |  0.9801 |   0.9804 \n",
      " 61 |  0.0447 | 0.3816 |  0.9806 |   0.9807 \n",
      "→ LR reduced to 1.000000e-04\n",
      " 62 |  0.0335 | 0.3810 |  0.9815 |   0.9816 *\n",
      " 63 |  0.0301 | 0.3811 |  0.9815 |   0.9816 \n",
      " 64 |  0.0290 | 0.3810 |  0.9814 |   0.9817 \n",
      " 65 |  0.0282 | 0.3810 |  0.9814 |   0.9816 \n",
      " 66 |  0.0275 | 0.3810 |  0.9814 |   0.9816 \n",
      " 67 |  0.0269 | 0.3811 |  0.9813 |   0.9815 \n",
      "→ LR reduced to 1.000000e-05\n",
      " 68 |  0.0263 | 0.3811 |  0.9812 |   0.9814 \n",
      "→ LR reduced to 1.000000e-06\n",
      " 69 |  0.0260 | 0.3811 |  0.9812 |   0.9814 \n",
      "→ LR reduced to 1.000000e-07\n",
      " 70 |  0.0261 | 0.3811 |  0.9812 |   0.9814 \n",
      "→ LR reduced to 1.000000e-08\n",
      " 71 |  0.0262 | 0.3811 |  0.9812 |   0.9814 \n",
      "→ LR reduced to 1.000000e-09\n",
      " 72 |  0.0261 | 0.3811 |  0.9812 |   0.9814 \n",
      "→ LR reduced to 1.000000e-09\n",
      " 73 |  0.0261 | 0.3811 |  0.9812 |   0.9814 \n",
      "→ LR reduced to 1.000000e-09\n",
      " 74 |  0.0261 | 0.3811 |  0.9812 |   0.9814 \n",
      "→ LR reduced to 1.000000e-09\n",
      " 75 |  0.0261 | 0.3811 |  0.9812 |   0.9814 \n",
      "→ LR reduced to 1.000000e-09\n",
      " 76 |  0.0260 | 0.3811 |  0.9812 |   0.9814 \n",
      "→ LR reduced to 1.000000e-09\n",
      " 77 |  0.0261 | 0.3811 |  0.9812 |   0.9814 \n",
      "→ LR reduced to 1.000000e-09\n",
      " 78 |  0.0260 | 0.3811 |  0.9812 |   0.9814 \n",
      "→ LR reduced to 1.000000e-09\n",
      " 79 |  0.0260 | 0.3811 |  0.9812 |   0.9814 \n",
      "→ LR reduced to 1.000000e-09\n",
      " 80 |  0.0262 | 0.3811 |  0.9812 |   0.9814 \n",
      "→ LR reduced to 1.000000e-09\n",
      " 81 |  0.0261 | 0.3811 |  0.9812 |   0.9814 \n",
      "→ LR reduced to 1.000000e-09\n",
      " 82 |  0.0261 | 0.3811 |  0.9812 |   0.9814 \n",
      "→ Early stopping @ epoch 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A%|██████████████████████████████████████████████████████████████████████████████████| 2/2 [35:22<00:00, 1066.57s/it]\n",
      "                                                                                                                       "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parallel(n_jobs=1)(delayed(train_gat_for)(n_layers,target_params, lth = layers_to_hyperparams_2, path_data = PATH_DATA3)\n",
    "                    for n_layers in tqdm([2,3,4], leave = False)\n",
    "                    for target_params in tqdm([100000, 500000,1000000], leave = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd86c188-8b96-4b70-81fd-76c5ea66a1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 39.29it/s]\n"
     ]
    }
   ],
   "source": [
    "PICKLE_DIR3 = \"./data/00.03\" \n",
    "\n",
    "\n",
    "records_3 = []\n",
    "\n",
    "print('here')\n",
    "for path in tqdm(glob.glob(os.path.join(PICKLE_DIR3, \"*.pkl\"))):\n",
    "    run = joblib.load(path)\n",
    "    stats_df = run['stats'].copy()\n",
    "    ep = stats_df['val_loss'].idxmin()\n",
    "    name = os.path.splitext(os.path.basename(path))[0]\n",
    "    records_3.append({'run': name, 'best_epoch': ep, 'test_accuracy': stats_df.loc[ep, 'acc_test'],\n",
    "                   'validation_accuracy': stats_df.loc[ep, 'acc_val']})\n",
    "    \n",
    "summary = pd.DataFrame(records_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05512f99-3313-4de4-9686-af292c50550e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>best_epoch</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>validation_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gat_2_100000</td>\n",
       "      <td>109</td>\n",
       "      <td>0.971456</td>\n",
       "      <td>0.970404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gat_2_1000000</td>\n",
       "      <td>84</td>\n",
       "      <td>0.977348</td>\n",
       "      <td>0.976752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gat_2_500000</td>\n",
       "      <td>102</td>\n",
       "      <td>0.977895</td>\n",
       "      <td>0.977084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gat_3_100000</td>\n",
       "      <td>129</td>\n",
       "      <td>0.978504</td>\n",
       "      <td>0.977840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gat_3_1000000</td>\n",
       "      <td>87</td>\n",
       "      <td>0.977994</td>\n",
       "      <td>0.977495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gat_3_500000</td>\n",
       "      <td>121</td>\n",
       "      <td>0.980475</td>\n",
       "      <td>0.980034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gat_4_100000</td>\n",
       "      <td>88</td>\n",
       "      <td>0.981028</td>\n",
       "      <td>0.980677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gat_4_1000000</td>\n",
       "      <td>61</td>\n",
       "      <td>0.981645</td>\n",
       "      <td>0.981479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gat_4_500000</td>\n",
       "      <td>82</td>\n",
       "      <td>0.974438</td>\n",
       "      <td>0.973589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             run  best_epoch  test_accuracy  validation_accuracy\n",
       "0   gat_2_100000         109       0.971456             0.970404\n",
       "1  gat_2_1000000          84       0.977348             0.976752\n",
       "2   gat_2_500000         102       0.977895             0.977084\n",
       "3   gat_3_100000         129       0.978504             0.977840\n",
       "4  gat_3_1000000          87       0.977994             0.977495\n",
       "5   gat_3_500000         121       0.980475             0.980034\n",
       "6   gat_4_100000          88       0.981028             0.980677\n",
       "7  gat_4_1000000          61       0.981645             0.981479\n",
       "8   gat_4_500000          82       0.974438             0.973589"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7f32ede-ddaf-45aa-8509-8011b244e791",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary[['prefix', 'layers', 'params']] = summary['run'].str.split('_', expand=True)\n",
    "summary['layers'] = summary['layers'].astype(int)\n",
    "summary['params'] = summary['params'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "666ebfbf-36be-429e-b60f-7443b0cbc9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = summary[['layers', 'params', 'test_accuracy', 'validation_accuracy']].rename(\n",
    "    columns={\n",
    "        'layers': '# layers',\n",
    "        'params': '#params',\n",
    "        'validation_accuracy': 'val_accuracy'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05005f99-a23c-484c-ad23-0b9ec0d98a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># layers</th>\n",
       "      <th>#params</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>1000000</td>\n",
       "      <td>0.981645</td>\n",
       "      <td>0.981479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.981028</td>\n",
       "      <td>0.980677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.980475</td>\n",
       "      <td>0.980034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.978504</td>\n",
       "      <td>0.977840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1000000</td>\n",
       "      <td>0.977994</td>\n",
       "      <td>0.977495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.977895</td>\n",
       "      <td>0.977084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1000000</td>\n",
       "      <td>0.977348</td>\n",
       "      <td>0.976752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.974438</td>\n",
       "      <td>0.973589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.971456</td>\n",
       "      <td>0.970404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   # layers  #params  test_accuracy  val_accuracy\n",
       "7         4  1000000       0.981645      0.981479\n",
       "6         4   100000       0.981028      0.980677\n",
       "5         3   500000       0.980475      0.980034\n",
       "3         3   100000       0.978504      0.977840\n",
       "4         3  1000000       0.977994      0.977495\n",
       "2         2   500000       0.977895      0.977084\n",
       "1         2  1000000       0.977348      0.976752\n",
       "8         4   500000       0.974438      0.973589\n",
       "0         2   100000       0.971456      0.970404"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df = output_df.sort_values(by = 'test_accuracy', ascending = False)\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec491bfe-4dc3-4a34-a358-c0752c3f0a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 'gat_performance.csv'\n",
    "output_df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36bd37f-018d-4bba-b269-98307065c4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
